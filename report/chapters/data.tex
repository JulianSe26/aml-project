% !TEX root =  master.tex
\chapter{Data}\label{chapter:data}

\section{NIH Data}\label{data:nih}
\sectionauthor{Written by Tobias Richstein}

The first of the datasets that we used to train our models comes from the \acf{NIH}, a division of the U.S. Department of Health, and is a collection of over 100 thousand chest X-ray images by over 30 thousand patients presented by \citeauthor{wang_chestx-ray8_2017} in \autocite{wang_chestx-ray8_2017}. This dataset was published in 2017 and is therefore not concerned with COVID-19 patients at all. Rather it consists of images depicting 14 different illnesses and images of healthy patients. The authors claim that there is a large amount of data in the form of patient \acp{CXR} and corresponding findings available in hospital's archiving systems but that these have not been properly consolidated and catalogued across multiple hospitals and states before. The authors also claim that the findings are mostly embedded in sentences making them not easily machine-readable.

To overcome these issues, the authors collected the images from different hospitals and used natural language processing to extract the medical findings from the written reports associated with the images. In the initial dataset this included eight different illnesses: Atelectasis, Cardiomegaly, Effusion, Infiltration, Mass, Nodule, Pneumonia and Pneumothorax as well as images with no finding at all. Later, the dataset was updated to the 15 class form (14 illnesses and no findings) that we use for our project to also include Consolidation, Edema, Emphysema, Fibrosis, Pleural Thickening and Hernia as findings. The class distribution can be seen in figure \vref{fig:nih_classes} where it is clear that the classes are very disproportionally represented. There is a total of \num{141537} diagnoses, meaning each image has an average of roughly $1.26$ labels associated. This number is skewed however since if anything is found then the average goes up to \num{1.56} since no finding always means that only this one label is attached to a record.

\begin{figure*}
	\centering
	\includegraphics[width=.8\linewidth]{img/nih_class_distribution.png}
	\caption{Class distribution in the NIH dataset}
	\label{fig:nih_classes}
\end{figure*}

The images in the dataset are given in the PNG format and are all sized $1024 \times 1024$ and with RGB color channels. The dataset does have roughly six thousand entries for bounding boxes for some of the images but those are not really of use to us since there are so few and also because the value of this dataset lies somewhere else for us: The training of our backbone network for one of the object detection networks (more on that in section \vref{chapter:rcnn}). We can use this dataset to train the backbone on images that are roughly within the problem domain that we actually want to tackle so that the feature vectors that it produces are valuable to the actual object detection network. 

\section{RSNA Data}\label{data:rsna}
\sectionauthor{Written by Julian Seibel}

Similar to the pre-training of the backbone on the above described NIH dataset, we use the \ac{RSNA} dataset \autocite{RSNAKaeggle} originally published as part of a two-stage Kaggle challenge in 2018. The dataset contains \num{30227} samples from \num{26684} different patients in the form of $1024 \times 1024$ DICOM images. Despite the data being anonymized, there is some metadata available for every patient including sex, age and radiographic view. The images were labeled by radiologists from the \ac{RSNA} and the Society of Thoracic Radiology. There are three mutually exclusive labels possible (lung opacity, normal, no lung opacity/ not normal) whereas lung opacity indicates evidence for pneumonia including also bounding box information for regions of interest in the format $(x_{\text{max}}, y_{\text{min}}, \text{width}, \text{height})$. An image that is labeled positively can contain multiple bounding boxes. Figure \ref{fig:rsna_classes} shows the class distribution for the \ac{RSNA} dataset. In total there are \num{9555} sample images that are labeled as positive and therefore come along with labeled regions in the form of bounding boxes.

The objective of this challenge was to train a model that is capable of correct classification including also bounding box predictions for each positive case and a confidence score, indicating the model confidence for a single bounding box prediction. The final submissions were evaluated using \ac{IoU} based metrics to measure the overlap of predicted and ground truth boxes. Figure \ref{fig:rsna_sample} shows an image from the \ac{RSNA} dataset including ground truth boxes (green), possible predictions (red-dashed) and corresponding \ac{IoU} values.

Since this challenge has a lot in common with the challenge we want to contribute to in this work and the image appearance is the exact same, namely DICOM images of \ac{CXR}, we decided to include this dataset as an additional source for pre-training our models. In contrast to the \ac{NIH} dataset, the \ac{RSNA} data is rather small, but brings one major advantage with labels including ground truth bounding boxes for positive pneumonia cases which makes the data suitable for pre-training object detection models. With this, we also think there are semantic similarities in predicting pneumonia and COVID-19 regions, that we hope will positively affect our final model performance for ultimately detecting COVID-19 symptoms. In detail, we use the positive cases for training our detection models to \enquote{introduce} the weights to the domain of medical X-ray chest images and the task of finding possible regions of interest evidencing typical patterns of a certain disease. For the metadata of the patients however, we did not find any suitable use which is why we ignored this information in the further course of this work.

\begin{figure*}
	\begin{minipage}[b]{.65\linewidth} % [b] => Ausrichtung an \caption
		\includegraphics[width=\linewidth]{img/rsna_class_distribution.png}
		\caption{Class distribution in the RSNA dataset}
		\label{fig:rsna_classes}
	\end{minipage}
	\begin{minipage}[b]{.35\linewidth} % [b] => Ausrichtung an \caption
		\includegraphics[width=\linewidth]{img/rsna_sample.png}
		\caption{Extracted from \autocite{rsnaSurvey}}
		\label{fig:rsna_sample}
	\end{minipage} 
\end{figure*}

\section{SIIM COVID-19 Data}\label{data:siim}
\sectionauthor{Written by Torben Krieger}

The last dataset we use is provided by the SIIM Kaggle Challenge itself \autocite{SIIMKaggle}. It contains \ac{CXR} images of COVID-19 positive and negative patients. The dataset itself is a reassembly of at least two previously published datasets. Namely the \textit{BIMCV-COVID19 Data} initially published by \citeauthor{vaya2020bimcv} in June 2020 \autocite{vaya2020bimcv} and the \textit{RSNA MIDRC-RICORD Data} published by \citeauthor{tsai2021rsna} in January 2021 \autocite{tsai2021rsna}. Only the X-ray images were taken from both datasets while the annotations were recreated for the assembled dataset. Both datasets were refreshed and supplemented multiple times. The description of the Kaggle Challenge not specifies which versions were used for the assembly.

The initial version of the \textit{BIMCV-COVID19} dataset contains \num{1380} CR images and \num{885} DX images \autocite{vaya2020bimcv}. Generally we do not distinguish between the two classical X-ray modalities CR and DX, meaning that we use all \num{2265} \ac{CXR} images. The images were taken from \num{1311} unique COVID-19 positive patients in hospitals in the region of Valencia in Spain. All images are provided in the DICOM format including metadata, which however is anonymized for privacy reasons. Nevertheless patients' age and sex was retained. The original dataset includes additional data, e.g. the results of one or multiple COVID-19 tests (where at least one is positive) and a corresponding radiological report.

The \textit{MIDRC-RICORD} dataset contains \num{1000} CXR images of \num{361} unique COVID-19 positive patients \autocite{tsai2021rsna}. Where positive means that at least one \ac{RT-PCR} test was positive for the corresponding patient. The data was collected from four different hospitals in Istanbul, San Francisco, Toronto and SÃ£o Paulo and is provided in DICOM format including metadata, which is, again, anonymized. Also here patient sex and age was retained and the authors specify that 148 out of the total amount of patients is female (41\%).

As stated above the Kaggle dataset is a combination of the datasets described before. As both datasets contain CXR images for positive tested patients only, there has to be at least one additional data source. Unfortunately the description of the challenge does not provide any details about this or the exact collocation of the assembled dataset. The new dataset was annotated by professional radiologists. Per image opacities were marked with bounding boxed if found. On the study level a classification into the categories, as described in section \vref{sec:kaggle}, was done by the radiologists. The classification follows the schema described by \citeauthor{litmanovich2020review} where each class is mutually exclusive \autocite{litmanovich2020review}. It is crucial to emphasize that radiologists had access to the COVID-19 status while creating the annotations \autocite{SIIMKaggleAnnotation}.

The dataset consists out of a train and test set, however the test labels are not published. A test is only possible by submitting a solution as a notebook. As we decided that we will not hand in our results, and use supervised methods only, we are limited to the train set for this report. The train set contains 6,334 images, where all of them are provided in the DICOM format. Metadata is available but anonymized. Out of the patient data only the patient sex was retained. All DICOM image files of the train set have an uncompressed size on the disk of \num{108.2}GB after the download. The images belong to \num{6054} studies. The count of unique patients could not be extracted from the anonymized metadata and is not specified. The resolution of the images reaches from \num{846} to \num{4891} pixel rows and \num{1228} to \num{4891} pixel columns. All images are single channel greyscale. The DICOM standard defines different representations for image pixel data, so called \textit{Photometric Interpretations} \autocite{dicom2018}. Within the dataset we could find \num{4633} images with the \texttt{MONOCHROME2} representation, which means that the minimum pixel value should be interpreted as black. Contrary \num{1701} images are encoded with \texttt{MONOCHROME1} representation where the minimum pixel value specifies white color. In DICOM, modalities can store a custom amount of bits per pixel value, which relates to the bit depth of the greyscale channel in our case. Most images (\num{4311} samples) are captured with a bit width of 12 bits. All other images have a bit width between 8 (\num{401} occurrences) and 16 bits. For training it is required to convert the pixel data of all files to a single representation and bit depth. Specifically by reducing the bit depth we lose some information which might be relevant for good predictions. Most of the included DICOM files contain uncompressed pixel data, only 440 files hold the pixel data compressed using the JPEG Lossless format. As expected the DICOM metadata field specifying the examined body part indicates \enquote{CHEST} most often, although encoded in different languages (e.g. \enquote{THORAX}, \enquote{PECHO}). However some unexpected values are included, e.g. \num{57} files claim \enquote{SKULL} as the examined body part. We checked some of these files manually and it seems that this is rather a misconfiguration of the modality. All images checked have shown the chest of the screened patients. For the complete train dataset \num{2770} images were taken of female patients, \num{3564} of male patients.

\begin{figure*}
	\centering
	\includegraphics[width=.75\linewidth]{img/class_distrinution_study.png}
	\caption{Distribution of classes for studies in the train dataset.}
	\label{fig:study-class-distribution}
\end{figure*}

Most of the \num{6054} studies were classified as \textit{Typical Appearance} (\num{2855} occurrences). In contrast to this the class \textit{Atypical Appearance} has only \num{474} occurrences, which is roughly a sixth of the most frequent class. Thus the dataset is imbalanced with in regards to the distribution of potential COVID-19 positive samples. Figure \ref{fig:study-class-distribution} shows the complete class distribution of the train set. Per description of the challenge for the classes \textit{Typical} and \textit{Indeterminate} bounding boxes marking opacities should always be present \autocite{SIIMKaggleAnnotation}. Contrary to that for the negative class no bounding boxes were created. For \textit{Atypical Appearances} sometimes a bounding box is included. In total we found \num{2040} images without any bounding boxes/opacities. Figure \ref{fig:boxes-per-images} shows an overview of the count of boxes per image. Most were annotated with two opacities. The figure \ref{fig:box-sizes} prints the number of boxes over the box size, thus giving an impression of the box sizes within the images. Related to this the description says that the radiologists opted for a single larger bounding boxes when multiple adjacent opacities were found, rather than creating multiple smaller boxes \autocite{SIIMKaggleAnnotation}. Bounding boxes for the image level and classes per study are provided in two separate CSV files.

\begin{figure*}
		\centering
		\includegraphics[width=.75\linewidth]{img/box_distribution_image.png}
		\caption{Count of boxes for opacities found per image. Where \texttt{NaN} means no box/opacity.}
		\label{fig:boxes-per-images}
\end{figure*}

From the construction of the dataset itself we could already foresee potential issues with regards to the real generalization capability of trained models. Most importantly the dataset is constructed out of several sources, where specific sources provided positive or negative samples only. Thus it could be that a potential solution learns shortcuts introduced by the processes or equipment used by a specific image source. An example could be burned in image markers at a specific position of the X-ray. We were able to find some re-occurring burned-in markers at the same position for a couple of images within the dataset. The next problem is the fact that we do not know about the dataset, e.g. the age distribution or the position of the patients while capturing the \ac{CXR} scan. For example one could imagine that images retrieved in a lying position correspond to more severe and potentially positive cases. All the critical points described before were mentioned in a paper by \citeauthor{degrave2021ai} \autocite{degrave2021ai} which explicitly investigated the \textit{BIMCV-COVID19} dataset and found the aforementioned shortcomings therein. As the Kaggle dataset is a reassembly of this dataset we have to expect that these issues are also present in it. Additionally we only have very limited information about the construction process of the dataset. Summing up a comprehensive conclusion about the generalization capability of our methods will not be possible by using this dataset.

\begin{figure*}
	\centering
	\includegraphics[width=0.75\linewidth]{img/box_size_distribution.png}
	\caption{Count of boxes over box sizes for found opacities.}
	\label{fig:box-sizes}
\end{figure*}

\subsection*{Preprocessing of the dataset \& COCO}
Independently of the used model or task we first applied preprocessing to the dataset. In general we apply the following to each DICOM image file:
\begin{itemize}
	\item Applying VOI-LUT, if present within DICOM metadata. This is a necessary pixel value transformation for some modalities to transform pixel values retrieved by the X-ray detector to a specific representation for displaying the image \autocite{dicom2018}. Not all DICOM files come with a VOI-LUT attribute.
	\item Inverting pixel values for \texttt{MONOCHROME1} images. By doing so we make sure that all pixel values within the images comply to the \texttt{MONOCHROME2} representation, meaning the minimum value is interpreted as black.
	\item Rescale all greyscale pixel values to a value range of $[0,255]$ to represent them with a bit depth of 8 bit.
	\item Store each image as a losslessly compressed grayscale image using the \ac{PNG} file format.
\end{itemize}

By applying the above mentioned preprocessing steps we reduced the overall size of the dataset to 25 GB. We also dropped all DICOM metadata as we do not require this information for the training and evaluation process. Still it is worth mentioning again that the rescaling of all image pixel data to a bit depth of 8 bit implies a significant loss of image information. However as we require the pre-training of backbone models as well as object detector for this project this is definitely appropriate.

For the object detection task on the image-level we need a specific format of annotation to make use of several pre-defined model architectures and tools. This format is specific to the Microsoft \ac{COCO} dataset initially described by \citeauthor{lin_microsoft_2015} in \citeyear{lin_microsoft_2015}. This dataset is one of the largest and most commonly used datasets for object detection, segmentation and captioning tasks. To reuse some pre-trained models and their tools easily and also apply transfer learning for other tasks the custom dataset has to comply to the label format used in the \ac{COCO} dataset.

\begin{minipage}{\linewidth}
	\begin{lstlisting}[language=json,firstnumber=1, caption={Basic COCO annotation format in \autocite{lin_microsoft_2015}.}, captionpos=b, label={lst:coco-sample}]
{
  "images": [
    { "id": int, "width": int, "height": int, "file_name": str,"date_captured": datetime }, (..)
  ], 
  "annotations": [
    { "id": int, "image_id": int, "category_id": int, "area": float, "bbox": [x,y,width,height] }, (..)
  ],
  "categories": [
    { "id": int, "name": str }, (..)
  ]
}
	\end{lstlisting}
\end{minipage}

The label format is defined using JSON and listing \ref{lst:coco-sample} shows the most important content nodes and attributes for our task.
The node \texttt{images} contains one entry per image and the \texttt{file\_name} attribute is a path. The node \texttt{annotations} contains a single node per bounding box for any image.
We provide functionality to convert the image level CSV file provided by the Kaggle dataset into a single JSON file which conforms to the \ac{COCO} format.


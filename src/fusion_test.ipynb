{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import sys\r\n",
    "sys.path.append('./yolo')\r\n",
    "from detection_fusion import EnsembleModel\r\n",
    "from PIL import Image, ImageDraw\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from rcnn.dataset import ChestCocoDetection\r\n",
    "from torchvision import transforms\r\n",
    "import matplotlib.patches as patches\r\n",
    "from torch.utils.data import DataLoader\r\n",
    "import tqdm\r\n",
    "import torch"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from yolo.utils.general import xywh2xyxy, scale_coords, box_iou\r\n",
    "from yolo.utils.metrics import ap_per_class\r\n",
    "import torch\r\n",
    "from rcnn.model import ChestRCNN\r\n",
    "from yolo.yolo import Model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def add_bounding_boxes(target, ax):\r\n",
    "    boxes = target['boxes']\r\n",
    "    for box in boxes:\r\n",
    "        mp_box = patches.Rectangle((box[0], box[1]), box[2] - box[0], box[3] - box[1], edgecolor=\"r\", facecolor='none')\r\n",
    "        ax.add_patch(mp_box)\r\n",
    "\r\n",
    "def show_samples_for(test, train):\r\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10,8))\r\n",
    "    #fig.suptitle(f'Study: {samples[\"id\"].iloc[i]}')\r\n",
    "    for im_i, (img, data) in enumerate([test, train]):\r\n",
    "        ax = axs[im_i] if isinstance(axs, np.ndarray) else axs\r\n",
    "        ax.set_title(f'Instance / Image Nr. {im_i + 1} / {2}')\r\n",
    "        #dcm = pydicom.dcmread(image_path)\r\n",
    "        plt.figure()\r\n",
    "        ax.imshow(img.permute(1,2,0))#,cmap=plt.cm.bone)\r\n",
    "        add_bounding_boxes(data, ax)\r\n",
    "    fig.savefig(\"combo.jpg\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "test = ChestCocoDetection(root=\"F:\\\\aml-project\\data\\\\siim-covid19-detection\", ann_file=\"F:\\\\aml-project\\\\data\\\\siim-covid19-detection\\\\test.json\", training=False, image_size=512)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "idx = np.random.randint(len(test))\r\n",
    "test_img = test.__getitem__(idx)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "YOLO_FINAL_MODEL_PATH = \"./yolo/models_final_giou_40/yolov5_epoch_25.pt\"\r\n",
    "FASTER_RCNN_FINAL_MODEL_PATH = \"./rcnn/models/fasterrcnn_epoch_23.pt\"\r\n",
    "RESNET_BACKBONE_PATH = \"./resnet/models/resnext101_32x8d_epoch_35.pt\"\r\n",
    "\r\n",
    "yolov5_weights = torch.load(YOLO_FINAL_MODEL_PATH)\r\n",
    "fasterrcnn_r101_weights = torch.load(FASTER_RCNN_FINAL_MODEL_PATH)\r\n",
    "\r\n",
    "yolo = Model(cfg=\"./yolo/yolo5l.yaml\",ch=3,nc=1)\r\n",
    "yolo.load_state_dict(yolov5_weights, strict=False) \r\n",
    "\r\n",
    "fasterRcnn = ChestRCNN(RESNET_BACKBONE_PATH)\r\n",
    "fasterRcnn.load_state_dict(fasterrcnn_r101_weights)\r\n",
    "ensemble = EnsembleModel(fasterRcnn=fasterRcnn, yolo=yolo)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "D:\\Anaconda\\envs\\aml\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "test_img[0].shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([3, 512, 512])"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "results, boxes, scores, pred_labels, yolo_box, yolo_scores, yolo_labels, fcnn_box, fcnn_scores, fcnn_labels  = ensemble.detection_fusion(test_img[0].detach().clone().unsqueeze_(0), extended_output=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 3, 512, 512])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "yolo_box"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[46.2336, 139.6736, 139.008, 281.2928],\n",
       " [296.2944, 241.664, 389.632, 321.9968]]"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "fcnn_box"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[276.224, 195.2256, 404.9408, 362.752], [33.2288, 111.7696, 129.28, 358.2976]]"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "boxes"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[276.224, 195.2256, 404.9408, 362.752], [33.2288, 111.7696, 129.28, 358.2976]]"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "from torchvision.ops import box_iou"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "box_iou(torch.tensor(yolo_box),test_img[1][\"boxes\"])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.41024],\n",
       "        [0.00000]], dtype=torch.float64)"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "box_iou(torch.tensor(fcnn_box),test_img[1][\"boxes\"])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.00000],\n",
       "        [0.72410]], dtype=torch.float64)"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "test_img[0].shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([3, 512, 512])"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "show_samples_for(test_img, (test_img[0], {\"boxes\":boxes}))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation of Ensemble method"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def collate_fn(batch):\r\n",
    "    imgs, targets = zip(*batch)\r\n",
    "    imgs = torch.stack(imgs)\r\n",
    "    return tuple((imgs, targets))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# need bs = 1 because model fusion cannot handle batches \r\n",
    "test = ChestCocoDetection(root=\"F:\\\\aml-project\\data\\\\siim-covid19-detection\", ann_file=\"F:\\\\aml-project\\\\data\\\\siim-covid19-detection\\\\test.json\", training=False, image_size=512)\r\n",
    "test_loader = DataLoader(test, batch_size=1, shuffle=False, pin_memory=False, num_workers=0, collate_fn=collate_fn)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "jdict, stats, ap, ap_class = [], [], [], []\r\n",
    "stats_yolo, stats_fcnn = [], []\r\n",
    "iouv = torch.linspace(0.5, 0.55, 1) # iou vector for mAP@0.5:0.95\r\n",
    "niou = iouv.numel()\r\n",
    "general_test_results = []\r\n",
    "for i, (images, targets) in enumerate(tqdm.tqdm(test_loader)):\r\n",
    "    nb, _, height, width = images.shape\r\n",
    "    assert nb == 1\r\n",
    "    _, boxes, scores, pred_labels, yolo_box, yolo_scores, yolo_labels, fcnn_box, fcnn_scores, fcnn_labels = ensemble.detection_fusion(images, extended_output=True)\r\n",
    "    boxes = torch.tensor(boxes)\r\n",
    "    yolo_box = torch.tensor(yolo_box)\r\n",
    "    fcnn_box = torch.tensor(fcnn_box)\r\n",
    "    gt_boxes = targets[0]['boxes']\r\n",
    "    labels = targets[0]['labels']\r\n",
    "    nl = len(labels)\r\n",
    "    tcls = labels[:].tolist() if nl else []  # target class\r\n",
    "\r\n",
    "    if len(boxes) == 0:\r\n",
    "        if nl:\r\n",
    "            stats.append((torch.zeros(0, niou, dtype=torch.bool), torch.Tensor(), torch.Tensor(), tcls))\r\n",
    "            stats_yolo.append((torch.zeros(0, niou, dtype=torch.bool), torch.Tensor(), torch.Tensor(), tcls))\r\n",
    "            stats_fcnn.append((torch.zeros(0, niou, dtype=torch.bool), torch.Tensor(), torch.Tensor(), tcls))\r\n",
    "\r\n",
    "        continue\r\n",
    "\r\n",
    "    correct = torch.zeros(len(boxes), niou, dtype=torch.bool)\r\n",
    "    correct_yolo = torch.zeros(len(yolo_box), niou, dtype=torch.bool)\r\n",
    "    correct_fcnn = torch.zeros(len(fcnn_box), niou, dtype=torch.bool)\r\n",
    "    \r\n",
    "    if nl:\r\n",
    "        detected = []\r\n",
    "        detected_yolo = []\r\n",
    "        detected_fcnn = []\r\n",
    "        tcls_tensor = labels[:]\r\n",
    "\r\n",
    "        # Per target class\r\n",
    "        for cls in torch.tensor([1]):\r\n",
    "            ti = (cls == tcls_tensor).nonzero(as_tuple=False).view(-1)\r\n",
    "            pi = (cls == torch.tensor(pred_labels)).nonzero(as_tuple=False).view(-1)\r\n",
    "            pi_yolo = (cls == torch.tensor(yolo_labels)).nonzero(as_tuple=False).view(-1)\r\n",
    "            pi_fcnn = (cls == torch.tensor(fcnn_labels)).nonzero(as_tuple=False).view(-1)\r\n",
    "\r\n",
    "            # Search for detections\r\n",
    "            if pi.shape[0]:        \r\n",
    "                ious, i = box_iou(boxes[pi], gt_boxes[ti]).max(1)  # best ious, indices\r\n",
    "                # Append detections - Ensemble\r\n",
    "                detected_set = set()\r\n",
    "                for j in (ious > iouv[0]).nonzero(as_tuple=False):\r\n",
    "                    d = ti[i[j]]  \r\n",
    "                    if d.item() not in detected_set:\r\n",
    "                        detected_set.add(d.item())\r\n",
    "                        detected.append(d)\r\n",
    "                        correct[pi[j]] = ious[j] > iouv  \r\n",
    "                        if len(detected) == nl: \r\n",
    "                            break\r\n",
    "                            \r\n",
    "            if pi_yolo.shape[0]:\r\n",
    "                ious_yolo, i_yolo = box_iou(yolo_box[pi_yolo], gt_boxes[ti]).max(1)\r\n",
    "                # Yolo\r\n",
    "                detected_set_yolo = set()\r\n",
    "                for j in (ious_yolo > iouv[0]).nonzero(as_tuple=False):\r\n",
    "                    d = ti[i_yolo[j]]  \r\n",
    "                    if d.item() not in detected_set_yolo:\r\n",
    "                        detected_set_yolo.add(d.item())\r\n",
    "                        detected_yolo.append(d)\r\n",
    "                        correct_yolo[pi_yolo[j]] = ious_yolo[j] > iouv  \r\n",
    "                        if len(detected_yolo) == nl: \r\n",
    "                            break\r\n",
    "                            \r\n",
    "            if pi_fcnn.shape[0]:\r\n",
    "                ious_fcnn, i_fcnn = box_iou(fcnn_box[pi_fcnn], gt_boxes[ti]).max(1)\r\n",
    "                # Faster R-CNN\r\n",
    "                detected_set_fcnn = set()\r\n",
    "                for j in (ious_fcnn > iouv[0]).nonzero(as_tuple=False):\r\n",
    "                    d = ti[i_fcnn[j]]  \r\n",
    "                    if d.item() not in detected_set_fcnn:\r\n",
    "                        detected_set_fcnn.add(d.item())\r\n",
    "                        detected_fcnn.append(d)\r\n",
    "                        correct_fcnn[pi_fcnn[j]] = ious_fcnn[j] > iouv  \r\n",
    "                        if len(detected_fcnn) == nl: \r\n",
    "                            break\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "        # Append statistics (correct, conf, pcls, tcls)\r\n",
    "        #print((correct.cpu(), torch.tensor(scores),torch.tensor(pred_labels), tcls))\r\n",
    "        stats.append((correct.cpu(), torch.tensor(scores),torch.tensor(pred_labels), tcls))\r\n",
    "        stats_yolo.append((correct_yolo.cpu(), torch.tensor(yolo_scores),torch.tensor(yolo_labels), tcls))\r\n",
    "        stats_fcnn.append((correct_fcnn.cpu(), torch.tensor(fcnn_scores),torch.tensor(fcnn_labels), tcls))\r\n",
    "\r\n",
    "for statistic in [stats, stats_yolo, stats_fcnn]:\r\n",
    "    statistic = [np.concatenate(x, 0) for x in zip(*statistic)] \r\n",
    "    if len(statistic) and statistic[0].any():\r\n",
    "        p, r, ap, f1, ap_class = ap_per_class(*statistic)\r\n",
    "        ap50, ap = ap[:, 0], ap.mean(1) \r\n",
    "        mp, mr, map50, m = p.mean(), r.mean(), ap50.mean(), ap.mean()\r\n",
    "        nt = np.bincount(statistic[3].astype(np.int64), minlength=1)\r\n",
    "\r\n",
    "        general_test_results.append({\r\n",
    "        \"precision\": p,\r\n",
    "        \"recall\": r,\r\n",
    "        \"ap\": ap,\r\n",
    "        \"f1\": f1,\r\n",
    "        \"ap_class\": ap_class,\r\n",
    "        \"ap\": ap,\r\n",
    "        \"ap50\": ap50,\r\n",
    "        \"mp\": mp,\r\n",
    "        \"mr\": mr,\r\n",
    "        \"map50\": map50,\r\n",
    "        \"map\": m\r\n",
    "    })\r\n",
    "\r\n",
    "    else:\r\n",
    "        nt = torch.zeros(1)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 859/859 [1:01:35<00:00,  4.30s/it]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "general_test_results"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'precision': array([     0.6018]),\n",
       "  'recall': array([     0.5035]),\n",
       "  'ap': array([    0.40922]),\n",
       "  'f1': array([    0.54828]),\n",
       "  'ap_class': array([1]),\n",
       "  'ap50': array([    0.40922]),\n",
       "  'mp': 0.6018030538468677,\n",
       "  'mr': 0.5034965034965035,\n",
       "  'map50': 0.40921655619713104,\n",
       "  'map': 0.40921655619713104},\n",
       " {'precision': array([    0.60309]),\n",
       "  'recall': array([    0.50328]),\n",
       "  'ap': array([    0.40929]),\n",
       "  'f1': array([    0.54868]),\n",
       "  'ap_class': array([1]),\n",
       "  'ap50': array([    0.40929]),\n",
       "  'mp': 0.6030946457346565,\n",
       "  'mr': 0.5032776105936924,\n",
       "  'map50': 0.40928837471026075,\n",
       "  'map': 0.40928837471026075}]"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "general_test_results[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'precision': array([    0.60308]),\n",
       " 'recall': array([    0.51875]),\n",
       " 'ap': array([    0.41317]),\n",
       " 'f1': array([    0.55775]),\n",
       " 'ap_class': array([1]),\n",
       " 'ap50': array([    0.41317]),\n",
       " 'mp': 0.6030840801650881,\n",
       " 'mr': 0.5187539732994279,\n",
       " 'map50': 0.41316756652897707,\n",
       " 'map': 0.41316756652897707}"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "general_test_results[1]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'precision': array([    0.49427]),\n",
       " 'recall': array([    0.30196]),\n",
       " 'ap': array([    0.22094]),\n",
       " 'f1': array([    0.37489]),\n",
       " 'ap_class': array([1]),\n",
       " 'ap50': array([    0.22094]),\n",
       " 'mp': 0.4942709827518029,\n",
       " 'mr': 0.301963742380426,\n",
       " 'map50': 0.22093770323238282,\n",
       " 'map': 0.22093770323238282}"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "general_test_results[2]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'precision': array([    0.60309]),\n",
       " 'recall': array([    0.50328]),\n",
       " 'ap': array([    0.40929]),\n",
       " 'f1': array([    0.54868]),\n",
       " 'ap_class': array([1]),\n",
       " 'ap50': array([    0.40929]),\n",
       " 'mp': 0.6030946457346565,\n",
       " 'mr': 0.5032776105936924,\n",
       " 'map50': 0.40928837471026075,\n",
       " 'map': 0.40928837471026075}"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('aml': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "interpreter": {
   "hash": "e2c56da327a3544e8df7760b824cf0444fbff4a4b0d34d9cc09299f87e272f6b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
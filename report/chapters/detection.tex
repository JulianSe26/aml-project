% !TEX root =  master.tex
\chapter{COVID-19 Detection}\label{chapter:detection}


\section{Faster R-CNN}\label{chapter:rcnn}
\sectionauthor{Written by Tobias Richstein}

The Faster \acf{R-CNN} network architecture proposed in \autocite{ren_faster_2016} by \citeauthor{ren_faster_2016} is an evolutionary step in a line of \acp{R-CNN} which are \acp{CNN} that can perform object detection on images. When given an image, an \ac{R-CNN} is able to predict bounding boxes for detected objects and also classify them. Each predicted bounding box is also given a confidence score that expresses how reliable the model assumes this result is. State of the art Faster \acp{R-CNN} achieve \ac{mAP} scores with a \num{0.5} \ac{IoU} threshold of \num{0.484} on a reference COCO object detection validation set making it very well suitable for all kinds of detection tasks such as the one at hand. 

The original \ac{R-CNN} was proposed by \citeauthor{girshick_rich_2014} in \autocite{girshick_rich_2014}. This original \ac{R-CNN} consists of three modeules: The first one generates category independent region proposals which are regions in the image that the network believes could have relevant objects in them. In theory \ac{R-CNN} is agnostic to which network performs these region proposals but in the paper a method called \textit{Selective Search} \autocite{uijlings_selective_2013} is used to generate \num{2000} region proposals. In the second module the contents of these proposed bounding boxes are passed to a backbone network which in the paper was an \textit{AlexNet} CNN \autocite{krizhevsky_imagenet_2017}, but could be any suitable network architecture to generate a 4096-length feature vector. Then the feature vector is passed onto the third module which is a set of \acp{SVM}, each trained on one specific class, to predict the class of the object encompassed by the bounding box.

The approach described in \autocite{girshick_rich_2014} does work fairly well but has some big drawbacks. First it requires training of the backbone CNN and \ac{SVM} classifiers and then a separate training of the region proposal network, making it very slow to train. Another issue with this architecture was, that inference was very slow with images taking multiple seconds to be processed on a GPU which is most often not sufficient for any sort of time requirements that object detection tasks may have.

To overcome the issues of the original \ac{R-CNN} paper, Fast \ac{R-CNN} was proposed a year later in \autocite{girshick_fast_2015}. Now, instead of passing each proposed region through the CNN separately, the entire image is processed once for which the \ac{CNN} generates a feature map that is valid across all regions. Again using any \ac{CNN} backbone works, but the authors used the then state of the art \textit{VGG16} architecture \autocite{simonyan_very_2015}. While this approach does make the network faster, it still requires an external region proposal network to feed the Fast \ac{R-CNN} with proposals and the image. Some speedup is achieved by being able to train the classifier and bounding box regressor at the same time.

In a third advancement the concept of the Faster \ac{R-CNN} is introduced in \autocite{ren_faster_2016}. The most noticeable change is that the region proposal network is now built in and no longer requires using Selective Search or other methods. After the backbone convolution the feature maps can now be passed onto both the region proposal network and the classifier which means that they share the results of the convolution making the network faster. In the original paper, the authors use the same \textit{VGG16} backbone as in the Fast \ac{R-CNN} paper but note that a larger \textit{ResNet}\autocite{he_deep_2015} model might lead to better results at the cost of more compute intensive training and inference.

The hint that a \textit{ResNet} architecture might be the better backbone to use with a Faster \ac{R-CNN}, led us to research these kinds of networks. After \citeauthor{krizhevsky_imagenet_2017} introduced the widely acclaimed deep \ac{CNN} \textit{AlexNet} a trend started to make these sorts of networks ever deeper, using more and more convolution layers under the assumption that more layers would lead to the detection of finer and maybe more hidden features in images. In \autocite{he_deep_2015} however, \citeauthor{he_deep_2015} show that this assumption only holds to a certain degree and show that a 20 layer network can perform much better than the same network with 56 layers. There are many proposed theories why this might be but the authors focus on fixing it by introducing a so called \textit{Residual Block} which essentially passes the input and output of a layer onto the next layer by adding a shortcut identity mapping from input to output.  Also so called bottlenecks are used which perform a dimensionality reduction with $1 \times 1$ convolutions. In doing so the authors are able to train networks that are hundreds or thousands of layers deep while improving classification metrics with each layer added.

Building upon \textit{ResNet}, \citeauthor{xie_aggregated_2017} propose \textit{ResNeXt} in \autocite{xie_aggregated_2017}. This network architecture introduces the concept of cardinality $C$ where a residual ResNet block is split into $C$ identical groups of operations called paths. ResNeXt networks are described by the number of layers they have, their cardinality and the size of their bottleneck. The larger each parameter is, the more computationally intensive. As a middle ground we picked a model with 101 layers, a cardinality of 32 and a bottleneck size of 8. This is referred to as a \texttt{ResNeXt 101 32x8d}.

\subsection*{Training of the backbone}

First we trained the ResNeXt network to have a performant backbone that the Faster \ac{R-CNN} can utilize. A reference ResNeXt model architecture and implementation can be obtained directly from the makers of PyTorch \autocite{pytorch_team_resnext_nodate}, which we did. This reference implementation has also been pre-trained on the ImageNet dataset \autocite{deng_imagenet_2009}, meaning that we only fine-tune the weights to our use-case. We train the model on the NIH dataset described in section \vref{data:nih} and only expect it to predict the classes of illnesses that can be seen in the X-rays. We encode the ground truths, consisting of the 14 classes of the NIH dataset (plus one for \textit{No Finding}), as one-hot vectors and therefore also expect output tensors of the same dimension. Like in the original ResNeXt paper, we also use a \acf{SGD} optimizer that has Nesterov acceleration during training. Our learning rate decays over time and follows the equation given below which was originally proposed in \autocite{he_bag_2018} and modified slightly to provide a learning rate floor of $0.05 * \text{lr}_\text{initial}$: 

\begin{align}\label{eq:scheduler}
\text{lr}_t = \left(\frac{1}{2}\left(1 + \cos\left(\frac{t * \pi}{T}\right)\right) * 0.95 + 0.05\right) * \text{lr}_\text{initial}
\end{align}

where $t$ is the current learning rate scheduler step and $T$ is the total number of epochs. We take a step every other epoch and start with a learning rate of $\text{lr}_\text{initial} = 0.001$ (see also figure \ref{fig:lr_schedule}).

\begin{figure*}[h!]
	\centering
	\includegraphics[width=.6\linewidth]{img/LR.png}
	\caption{Exemplary learning rate schedule for 35 epochs applied during training of the detection models.}
	\label{fig:lr_schedule}
\end{figure*}

As described in the ResNeXt paper we load the images and then perform the augmentations necessary to fit the model requirements. To do so, we use a custom dataloader that provides batches of images together with the one-hot encoded ground truth vectors. The augmentation steps done during dataloading include:

\begin{itemize}
	\item Resize the image to have 256 pixels on the shorter side
	\item Perform a $224 \times 224$ crop in the center of the resized image
	\item Normalize the RGB channels in range $0$ to $1$ to have a mean of $R=0.485; G=0.456; B=0.406$ and a standard deviation of $R=0.229; G=0.224; B=0.225$
\end{itemize}

To prevent overfitting during training and essentially enlargen our dataset we also randomly apply additional augmentations such as horizontal flips ($p=0.5$), random blurs ($p=0.3$), random rotations of up to 20Â° ($p=1$) or random erasing of between 1 and 10 percent of the image area ($p=0.3$).

Since we have somewhat limited hardware resources at our disposal in comparison to large scale compute clusters that are often used for such training tasks by researchers, we also apply a method called \textit{Autocasting} to speed up training and allow us to use larger batch sizes. The basis of Autocasting is the ability to use mixed precision during network training. While most frameworks such as PyTorch usually use 32bit floating point numbers (single precision) for all calculations, it has been shown that performing some operations with 16bit representations (half precision) does not penalize accuracy but provides a large speedup since more data can fit in the most often constrained GPU memory and the also constrained data transfer bandwidth can be used more effectively \autocite{micikevicius_mixed_2018}. The GPUs that we have at our disposal also feature special matrix multiplication hardware that works best with half precision numbers, meaning that we profit from mixed precision training in a significant way. The speedup for the ResNeXt training for example was almost twice as fast as before. The decision whether to perform operations at half precision is made automatically by PyTorch when the model is wrapped in an autocasting decorator.

We train the ResNeXt with a batch size of 32 (like in the original paper) and perform 35 epochs. To calculate the loss we use Binary Cross Entropy but with Logits as recommended for mixed precision training which uses the log-sum-exp trick to improve the numerical stability and avoid loss terms that cannot be represented by half precision \autocite{pytorch_team_automatic_nodate}. The loss numbers for the training and validation loss can be seen in \vref{fig:resnet_loss}. It can be seen that in the end some overfit occurs where the train loss keeps decreasing and the validation loss stays mostly constant or even increases very slightly. In the end we still decided to use the model after 35 epochs since the loss figures are very good and it also evaluates very well as will be shown later in chapter \vref{chapter:eval_resnext}.

\begin{figure*}
	\centering
	\includegraphics[width=.7\linewidth]{img/loss_backbone_rcnn_35.png}
	\caption{Loss figures of the ResNeXt training}
	\label{fig:resnet_loss}
\end{figure*}

\subsection*{Training of the Faster R-CNN}

With the backbone network trained, we could now train the Faster R-CNN on the actual detection task of predicting where lung opacities are located in a patient's \ac{CXR} image. This training shares a lot of optimizations with the backbone network described above. We use the same \ac{SGD} optimizer and learning rate schedule and train for 50 epochs which does not take too long due to the limited number of training images. We also again use autocasting since the speed improvements are too good to leave out.

Due to the limited number of samples available in the SIIM dataset, we now augment the images more extensively to further prevent overfitting. Because we now have bounding boxes in the aforementioned \ac{COCO} format we also have to apply all augmentations to those too. To also allow the network to better detect small opacities and details we now train with a much larger image size of $512 \times 512$. We also perform random horizontal flips ($p=0.3$), random shifts with rotations of maximum 20Â° ($p=0.3$), one of random sharpen ($p=0.5$) or blur ($p=0.25$), random brightness and contrast adjustments ($p=0.3$) and random circular cutouts (max. 6; $p=0.3$). During inference however we pass the inputs as $1024 \times 1024$ images to make the results even clearer. As with the backbone net, we also adjust the RGB channels to fit the required mean and standard deviation values.

Due to the much larger input images and network size we can only train the Faster R-CNN with a batch size of 10 and perform validation with a batch size of 6. During training of a Faster R-CNN multiple loss values have to be taken into account since there are the two tasks of classification and bounding box prediction. Detailed loss figures can be seen in figure \vref{fig:rcnn_loss}. As will be evidenced later in chapter \vref{chapter:eval_rcnn_yolo} after 50 epochs there was already some overfit even though the loss numbers look promising.

\begin{figure*}
	\centering
	\includegraphics[width=.7\linewidth]{img/loss_fasterrcnn_50.png}
	\caption{Loss figures of the Faster R-CNN training}
	\label{fig:rcnn_loss}
\end{figure*}

Per default a Fast(er) R-CNN uses a smooth L1 loss for the box regression as described in \autocite{girshick_fast_2015} which according to the authors prevents exploding gradients unlike loss functions proposed in earlier R-CNN revisions. However, to try and improve convergence speeds and model accuracy, we also implemented a \ac{CIoU} loss, proposed in \autocite{zheng_enhancing_2021} and described in more detail in section \vref{chapter:yolo}, for the regressor. Unfortunately this did not work at all and the model converged a lot slower than anticipated and sometimes even became a lot worse over time. The reasons for this would need to be investigated further but due to time constraints we had to revert back to using the default smooth L1 loss function which in the end also proved quite capable as will be shown later in the evaluation in section \vref{chapter:eval_rcnn_yolo}.

\section{YOLO}\label{chapter:yolo}
\sectionauthor{Written by Julian Seibel}

The \acf{YOLO} model originally proposed in \autocite{yoloOriginal} is an object detector introduced in 2015 by \citeauthor{yoloOriginal}. In contrast to the previously presented Faster \ac{R-CNN}, this model makes its predictions with just a single network evaluation and is therefore called a single-shot detector (hence the name \ac{YOLO}). Unlike in region proposal or sliding window based network architectures, \ac{YOLO} considers the entire image when predicting which enables the model to implicitly encode context information about the objects. With this, the model is capable of learning the typical shape and size of objects, which objects are likely to occur together and what typical positions objects have in relation to other objects. The initial idea was to provide an object detection network that achieves both, high quality and high inference speed. The authors claim that their \ac{YOLO} model can be up to \num{1000} times faster than \ac{R-CNN} and up to \num{100} times faster than Faster \ac{R-CNN}.
Since its initial introduction, the \ac{YOLO} model was adapted in many research problems and has been improved in several follow-up works \autocite{yolov2} \autocite{yolov3} to finally come up with the newest version \textit{V4} \autocite{yolov4}.
In general, a \ac{YOLO} model consists of three main pieces:

\begin{itemize}
	\item The backbone, similar to the Faster \ac{R-CNN}, this is a deep \ac{CNN} that learns image features at different angularities. In their original paper, the authors used a backbone named \textit{Darknet} that is a neural network consisting of 53 convolution
	layers \autocite{yolov3}. However this was substituted with a \textit{CSPNet} \autocite{wang2020cspnet} since \textit{V4} \autocite{yolov4}
	\item The neck, which is a series of layers that combine features of different convolution layers. Since \textit{V4} the \textit{PANet} \autocite{tan2020efficientdet} neck is used for this part of the model 
	\item The head, that consumes the features from the neck and processes them further for the final box, confidence and class predictions
\end{itemize}

The \ac{YOLO} model divides each input image of size $512\times512$ into a $G\times G$ grid, where a grid cell is \enquote{responsible} for an object if it contains the object's center point. Each grid cell predicts bounding boxes using predefined anchors and corresponding confidence scores that indicate how likely it is that the box contains an object and how well the box fits to the object. For each bounding box a confidence score is predicted using logistic regression. Unlike in \acp{R-CNN}, the \ac{YOLO} model does not predict offset coordinates for predefined anchors, but rather predicts the location coordinates relative to the center of a grid cell which constrains the coordinates to fall between 0 and 1. This helps the network to learn the parameters more easily. Therefore, the model prediction for a bounding box is a quintuple $(t_x,t_y,t_w,t_h,c)$ consisting of four coordinates and one for the object confidence (also called \enquote{objectness}), where $t_x$ and $t_y$ are the normalized center coordinates of the bounding box. As illustrated in figure \ref{fig:yolo_box}, the model predicts the width and height of the bounding box as offsets from center coordinates of the box and anchors given the offset from the grid cell to the top left image corner ($c_x$,$c_y$) and the anchor width and height ($p_w$,$p_h$) \autocite{yolov2} \autocite{yolov3}.

\begin{figure*}[h!]
	\centering
	\includegraphics[width=.45\linewidth]{img/boxpred.png}
	\caption{\ac{YOLO} box prediction extracted from \autocite{yolov3}.}
	\label{fig:yolo_box}
\end{figure*}

For each predicted box a class label is assigned using a sigmoid based multi-label classification. The confidence score for an object is then the product of \enquote{objectness} and class confidence to express the probability of a certain class instance appearing in the predicted bounding box and the quality of how well the box fits on the object.
The model will produce outputs at different scales and depending on the number of anchors, for each grid cell multiple bounding boxes will be predicted. This often leads to a huge number of predictions per image, which is why non maximum suppression is performed to filter out boxes that do not meet a certain confidence threshold and that overlap to much in terms of \ac{IoU}.


\subsection*{Training of the \ac{YOLO} model}
For our implementation, we decided to use a model provided by \autocite{yolov5} which is unofficially called \ac{YOLO} \textit{V5} which is based on the \ac{YOLO} \textit{V4} with some improvements of speed and quality. For our model configuration we use the \textit{yolov5l} model according to \autocite{yolov5}.

Similar to \autocite{CoronaDLTransfer} and  \autocite{mangal2020covidaid}, we found that there is not enough data available to achieve state-of-the-art results which is why we use a transfer-learning approach with pre-trained weights on the \ac{COCO} \autocite{coco} dataset, leading to a performance boost in terms of our selected metrics (see \vref{chapter:eval_rcnn_yolo} for further details). Furthermore, we did a second pre-training step, where we used the RSNA pneumonia detection dataset described in \vref{data:rsna}, to adjust the initial \ac{COCO} trained weights in the direction of medical lung images by training the model for $30$ epochs. Despite the objective being different (detecting pneumonia instead of COVID-19), both datasets share many things like the prediction of just one class called \textit{opacity}. We could see an increased performance in the evaluation when we did such a pre-training. The final model is then ultimately trained for 35 epochs on the SIIM COVID-19 data.

As already mentioned in the Faster \ac{R-CNN} training section, we did several experiments implementing alternative regression losses for the bounding box predictions. In the \ac{YOLO} paper the authors use a \ac{MSE} regression loss to train the bounding box output. However, as \citeauthor{giou} \autocite{giou} report in their work, there is not a strong correlation between minimizing the \ac{MSE} and improving the \ac{IoU} value. Since a plain \ac{IoU}-based loss would not consider the actual distance of a prediction to a ground truth box, meaning that if there is no intersection, the \ac{IoU} value would be zero and not differentiate between predictions that are close to the ground truth and predictions that are far away from the ground truth.
In addition the authors put forth that the evaluation of the model is mostly done using \ac{IoU}-based metrics whereas the training is performed by minimizing a \ac{MSE} loss.

Therefore the authors propose to use a more appropriate loss which they call \ac{GIoU} loss that uses the smallest convex hull of both bounding boxes to encode their relationship also in terms of distance. There are several extensions like the previously described \ac{CIoU} loss \autocite{zheng_enhancing_2021} or \ac{DIoU} loss \autocite{DIoU}. For our final model, we experienced best results using the \ac{GIoU} loss proposed by \citeauthor{giou}.
The objectness and class confidence predictions were trained using a Binary Cross Entropy with Logits loss. The final loss is then a weighted sum of all the three single losses:
\begin{align}
	\mathcal{L}_{total} = 0.075 * \mathcal{L}_{box} + 0.05 * \mathcal{L}_{class} + 0.75* \mathcal{L}_{objectness}
\end{align} 

For the training process, we used a similar setup like in the Faster \ac{R-CNN} training including an \acs{SGD}-based optimizer with a starting learning rate of $lr_{initial} = 0.01$ and momentum of $0.937$, a learning rate schedule as described in equation \ref{eq:scheduler} and an input image size of $512 \times 512$. Because of VRAM constraints in our hardware, we could only use a batch size of three (excluding augmentations) but we used a gradient accumulation based on a nominal batch size of $64$, which has the effect that the optimizer and scaler only update if the nominal batch is reached rather than the limited physical batch.
Furthermore we applied \num{1000} warm-up iterations, weight decay of $0.0005$ following the approach of \autocite{yolov5} and similar to the Faster \ac{R-CNN} approach implemented the training process using autocasting for speed-up. After a forward pass we reduce the amount of possible bounding boxes using non maximum suppression with a confidence threshold of $0.1$ and an \acs{IoU} threshold of $0.2$.
Similar to the Faster \ac{R-CNN} we apply several image transformations as augmentation technique, which include:
\begin{itemize}
	\item Image transformations like flipping up-down (probability of $0.1$) and left-right (probability of $0.5$)
	\item Augmentation of the image and color space using different values for hue, saturation and values in the HSV space
	\item Mixup \autocite{zhang2017mixup} with a probability of $0.5$
\end{itemize}

The losses for the pre-training and actual COVID-19 training are illustrated in figure \ref{fig:yolo_losses}. For the pre-training, we can see a slight increase of the validation loss after about $25$ epochs where we assume it may be caused by overfitting (also visible in the corresponding validation metrics described in \ref{chapter:eval_rcnn_yolo}). The right side of the figure shows the loss and its components for the actual training on the COVID-19 data. It can be seen that also here the loss decreases over time, starting already at relatively low loss values. This and the fact that the losses do not decrease as steeply as in the previous training is mainly caused by the transfer learning approach using the pre-trained weights on the \ac{RSNA} data.
Additionally, the loss graph shows some uncommon patterns which we faced in all our experiments using the \ac{YOLO} model: The validation loss is always smaller than the train loss. Since we did not see any break-in of model performance in terms of metrics measured, we did not follow-up on this, since we could exclude any wrong learning or restrictions of the model. However, such patterns may be caused by:
\begin{itemize}
	\item The validation set being too small
	\item Regularization, e.g. dropout that is not active during testing or validation
	\item Some shifts due to the time of measurement, the train loss is obtained after every epoch whereas the validation is only measured after an epoch completes. This could cause the mean being pulled from bad performance at the very first iterations of each epoch
	\item Data augmentation which is only used during the training process but not included in the validation or testing process
\end{itemize}

\begin{figure}[h]
	\centering
	\begin{minipage}{.5\textwidth}
		\centering
		\includegraphics[width=\linewidth]{img/loss_yolo_30.png}
	\end{minipage}%
	\begin{minipage}{0.5\textwidth}
		\centering
		\includegraphics[width=\linewidth]{img/loss_yolo_35_siim_final.png}
	\end{minipage}
\caption{Train and validation loss for the pre-training on the RSNA datset (left) and the SIIM COVID-19 datset (right).}
\label{fig:yolo_losses}
\end{figure}


\section{Combining detections}\label{section:combining_detections}
\sectionauthor{Julian Seibel}
For our final COVID-19 detection, we decided to get the two described models in an ensemble predictor to combine both advantages of the models and also to create a more stable bounding-box predictor that considers both outputs. Following the approach of \citeauthor{weightedBoxFusion} \autocite{weightedBoxFusion}, we created a weighted box fusion, where we get a final bounding box prediction given the predicted boxes and confidence scores from the \ac{YOLO} and Faster \ac{R-CNN} models.

Each predicted box is added to a list $B$, that is sorted w.r.t. the corresponding confidence scores. Then two new empty lists are created for box clusters $L$ and fused boxes $F$. Each position in $F$ stores the fused box for the corresponding entry $pos$ in $L$. The algorithm then iterates over the predicted boxes in $B$ trying to find a matching box in $F$ based on an \acs{IoU} criterion (e.g. \acs{IoU} > threshold, where in our case the threshold was set to $0.55$). If no match is found, the box from list $B$ is added to $L$ and $F$. In contrast, if a match is found, the box is added to the cluster list $L$ at the corresponding position to the box in $F$. The box coordinates $(x,y)$ and confidence scores $c$ in $F$ will then be recalculated using all boxes accumulated in $L[pos]$ with the fusion equation:

\begin{align}
	c = \frac{\sum_{i=1}^{T}c_i}{T}
\end{align}
\begin{align}
	x_{1,2} = \frac{\sum_{i=1}^{T} c_i * x_{1,2}^i}{\sum_{i=1}^{T} c_i}
\end{align}
\begin{align}
	y_{1,2} = \frac{\sum_{i=1}^{T} c_i * y_{1,2}^i}{\sum_{i=1}^{T} c_i}
\end{align}

Using the confidence scores as weights, predicted boxes with higher confidence naturally contribute more to the fused box.
If all predicted boxes in $B$ are processed, confidence scores in $F$ will be re-scaled using the number of predicted bounding boxes $T$ and and the number of participating models $M$:

\begin{align}
	c = c * \frac{min(T,M)}{M}
\end{align}

In our final version, we set the weights for the box fusion $w_{fusion} = (1,1)$ meaning that each model contributes the same to the final prediction.
As fusion-criteria we set an \ac{IoU}-threshold for both predictions to $IoU_{fusion} = 0.55$ which was also the best experienced parameter value described in the original paper \autocite{weightedBoxFusion}. In a last step, we do again non maximum suppression of the fusion boxes to obtain our final ensemble result. 
We did not apply any training process to the ensemble model, but it would be interesting to train the ensemble approach in an end to end fashion. However, we do not have the computational resources available to jointly train both detection models.



\section{Study-Level model}
\sectionauthor{Written by Torben Krieger}

As described in \ref{sec:kaggle} the study level task can be seen as a multi-class classification. A study consists of one or more \ac{CXR} images, meaning that we could interpret the task as single-class image classification and combination of the results of each individual image afterwards. However we decided to skip this combination and perform the classification for single images only. Besides the fact that only a few number of studies consist of multiple images, a moderator of the challenge reported in a discussion thread that most often the reason for multiple images are quality issues of the other images(s) in the study \footnote{https://www.kaggle.com/c/siim-covid19-detection/discussion/240250\#1321539}.

\subsection*{Image Classification using ResNeXt}
A \ac{CNN} is the proper neural network structure to perform classification tasks on images. As the amount of training data available for the Kaggle Challenge is quite limited, we have to use a pre-trained model to utilize a recent \ac{CNN} network architecture for this task. Most available pre-trained models are trained on the ImageNet dataset, which consists of real world images \autocite{deng_imagenet_2009}. Thus we consider that a re-training of just the classification layer is not sufficient. However we also believe that even for a simple re-adaption of the feature extraction layers the provided dataset is too small. Consequently we think that a second pre-training of the model using the NIH dataset, as introduced in \ref{data:nih}, is necessary for this task too.

Initially we thought about using the output of our object detection model as an additional input or hint for the image classification model like using a mask or by a separate feature extraction model feeding into the same classification layers. However we came to the conclusion that we will not use the output. On the one hand the study level model would somehow inherit the errors of the detection models. On the other hand one could argue that the model itself should be capable of learning characteristics like opacities which indicate a potential COVID-19 infection on its own. Or in other words, the image classification model should be able to learn the same semantics as the object detection model when using the same data.

For simplification and to avoid re-doing the pre-training of the model on the NIH dataset we decided to reuse the trained backbone model of the Faster R-CNN, as described in \ref{chapter:rcnn}. Therefore the weights of the trained \texttt{ResNeXt 101 32x8d} model are loaded and the classification layer is replaced. The new classification layer consists of two fully connected layers intersected by a \textit{ReLU} activation function. The pooled output of the feature extraction layers have a dimensionality of 2048, which we reduced to 1024 with the first fully connected layer. The final layer consists of 4 neurons, each representing a single class. As the \textit{ResNeXt} was trained on ImageNet initially it requires RGB images with a size of $224 \times 224$ pixels as its input \autocite{xie_aggregated_2017}. 

\subsection*{Training of the ResNeXt for Image Classification}
For taking over the weights of the trained backbone \textit{ResNeXt} a new model was defined using PyTorch. This model consists of the same pre-built and trained \texttt{ResNeXt 101 32x8d} and the classification layers mentioned above. The classification layers are initialized by \textit{Xavier Initialization} \autocite{glorot2010understanding}. Afterwards a checkpoint of the trained backbone model is loaded into the model with the non-strict policy. This means even if parameters of the model are not contained within the checkpoint the import will not fail, which is required as the new classification layers are not part of the checkpoint.

Due to the usage of the \textit{ResNeXt} the same data augmentation as described in \ref{chapter:rcnn} is required. Additionally we apply the same training augmentations to the SIIM dataset for the study level training. The pre-training on the NIH dataset was done as a multi-label classification. As the study level task implies a multi-class classification task other normalization and loss functions are required. During the training of the \textit{ResNeXt} on the SIIM dataset we use \textit{Softmax} to normalize all outputs to a sum of 1 \autocite{goodfellow2016deep}. As the loss function we use \textit{Cross Entropy}, again for numerical stability we apply the combined PyTorch loss function working on logits. Apart from this the initial setup was similar to the backbone training. This means that \ac{SGD} was used as the optimizer and \textit{Cosine Annealing} for learning rate decay scheduling. 

After a first training run we noticed that even for a decreasing training loss the validation loss starts to increase after few epochs. We identified this generalization error as overfitting and tried to apply following regularization techniques as countermeasures (both separately or combined):
\begin{itemize}
	\item Adding a \textit{Dropout} layer between the last two fully connected layers and validating different probabilities for dropping output values with $0.5 \leq p \leq 0.7$
	\item Applying additional training data augmentations or executing them with higher probabilities.
	\item Using \textit{weight decay} to apply \textit{L2 regularization}
\end{itemize}

\textit{Weight decay} was applied as defined in equation \ref{eq:weight-decay}, where $\boldsymbol{\theta}$ represents all weights, $\alpha$ the learning rate and $\lambda$ the rate of decay each step. Effectively weight decay means that existing weights are decreased exponentially each update step. Thus the value of the weights decay to zero if not balanced by the weight update due to the gradient step. This method is comparable to adding a \textit{L2 regularization} term to the loss function when using \textit{SGD} \autocite{loshchilov2017decoupled} and penalizes large weights in practice.

\begin{align}\label{eq:weight-decay}
	\boldsymbol{\theta}_{t+1} = \left( 1 - \lambda \right) \boldsymbol{\theta}_{t} - \alpha \nabla \text{Loss}_t \left( \boldsymbol{\theta}_{t} \right)
\end{align}

We tried several \textit{decay rates} in the range of $[0.00001, 0.3]$ but were unable to regularize the overfitting successfully. Figure \vref{fig:weight-decay} shows the training loss for some of the validated decay rates.

\begin{figure*}
	\centering
	\includegraphics[width=.7\linewidth]{img/reg_study_weight_decay.png}
	\caption{Validation losses for different weight decay rates. The runs were stopped early, thus the count of epoch per sample differs.}
	\label{fig:weight-decay}
\end{figure*}

Also the application of additional data augmentation was not successful. Instead the model was unable to improve the training loss for some setups. Besides that the dropout layer had nearly no effect. Due to the usage of a pre-trained model \textit{Dropout} could only be used easily within the custom layers forming the classifier. Applying \textit{Dropout} to other hidden layers of the pre-trained model requires a more complex change to the predefined model. Thus we decided to not try this out.
As an alternative to the mentioned regularization methods we disabled the weight updates for all layers within the pre-trained model, namely the feature extraction module. Performing updates for the classification layer only and reducing the amount of trainable parameters stopped the overfitting of the model successfully. Still we had some problems with fluctuating validation losses or even increasing training losses after an initial decrease. Thus we performed a manual hyper-parameter search. This was done by altering the training script to read in all parameters from a configuration file and creating a couple of reasonable combinations. We then executed all configurations within a script and compared results later on.

As result we came up with a training and validation loss as shown in figure \ref{fig:study-loss}. Still especially the validation loss is highly volatile and the overall decrease of the loss is relatively small. However compared to other validation losses reported for the hyper-parameter search this configuration reached the minimal validation loss. The training is executed for 58 epochs with a batch size of 15 and a learning rate $l_r = 0.0005$.

\begin{figure*}
	\centering
	\includegraphics[width=.7\linewidth]{img/loss_study_level_ep58.png}
	\caption{Training and validation loss for final training of the study level model.}
	\label{fig:study-loss}
\end{figure*}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60af1d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./yolo')\n",
    "from detection_fusion import detection_fusion\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from rcnn.dataset import ChestCocoDetection\n",
    "from torchvision import transforms\n",
    "import matplotlib.patches as patches\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffecaa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bounding_boxes(target, ax):\n",
    "    boxes = target['boxes']\n",
    "    for box in boxes:\n",
    "        mp_box = patches.Rectangle((box[0], box[1]), box[2] - box[0], box[3] - box[1], edgecolor=\"r\", facecolor='none')\n",
    "        ax.add_patch(mp_box)\n",
    "\n",
    "def show_samples_for(test, train):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10,8))\n",
    "    #fig.suptitle(f'Study: {samples[\"id\"].iloc[i]}')\n",
    "    for im_i, (img, data) in enumerate([test, train]):\n",
    "        ax = axs[im_i] if isinstance(axs, np.ndarray) else axs\n",
    "        ax.set_title(f'Instance / Image Nr. {im_i + 1} / {2}')\n",
    "        #dcm = pydicom.dcmread(image_path)\n",
    "        plt.figure()\n",
    "        ax.imshow(img.permute(1,2,0), cmap=plt.cm.bone)\n",
    "        add_bounding_boxes(data, ax)\n",
    "    fig.savefig(\"combo.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d89adcfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "test = ChestCocoDetection(root=\"F:\\\\aml-project\\data\\\\siim-covid19-detection\", ann_file=\"F:\\\\aml-project\\\\data\\\\siim-covid19-detection\\\\test.json\", training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "642b7530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.Size([3, 1024, 1024])\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(len(test))\n",
    "test_img = test.__getitem__(idx)\n",
    "print(type(test_img[0]), test_img[0].shape)\n",
    "img = test_img[0]\n",
    "img_pil = transforms.ToPILImage()(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "505bce10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\aml\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "results, boxes, scores = detection_fusion(img_pil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fcf7422",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    }
   ],
   "source": [
    "show_samples_for(test_img, (test_img[0], {\"boxes\":boxes}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036742ac",
   "metadata": {},
   "source": [
    "### Evaluation of Ensemble method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0649c768",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    imgs, targets = zip(*batch)\n",
    "    imgs = torch.stack(imgs)\n",
    "    return tuple((imgs, targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26bc1c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need bs = 1 because model fusion cannot handle batches \n",
    "test_loader = DataLoader(test, batch_size=1, shuffle=False, pin_memory=False, num_workers=1, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9785e95",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (Temp/ipykernel_8244/631558422.py, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\Julian\\AppData\\Local\\Temp/ipykernel_8244/631558422.py\"\u001b[1;36m, line \u001b[1;32m10\u001b[0m\n\u001b[1;33m    labels = targets[targets[:, 0] == si, 1:]\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "jdict, stats, ap, ap_class = [], [], [], []\n",
    "for images, targets in tqdm.tqdm(test_loader):\n",
    "    nb, _, height, width = images.shape\n",
    "    assert nb == 1\n",
    "    img_pil = transforms.ToPILImage()(img)[0]\n",
    "    _, boxes, scores, labels, _, _, _, _ = detection_fusion(img_pil)\n",
    "    \n",
    "     # Statistics per image\n",
    "    for si, pred in enumerate(out):\n",
    "    labels = targets[targets[:, 0] == si, 1:]\n",
    "    nl = len(labels)\n",
    "    tcls = labels[:, 0].tolist() if nl else []  # target class\n",
    "    path = Path(paths[si])\n",
    "    seen += 1\n",
    "\n",
    "    if len(pred) == 0:\n",
    "        if nl:\n",
    "            stats.append((torch.zeros(0, niou, dtype=torch.bool), torch.Tensor(), torch.Tensor(), tcls))\n",
    "        continue\n",
    "\n",
    "    # Predictions\n",
    "    pred[:, 5] = 0\n",
    "    predn = pred.clone()\n",
    "    scale_coords(imgs[si].shape[1:], predn[:, :4], shapes[si][0], shapes[si][1])\n",
    "\n",
    "\n",
    "    correct = torch.zeros(pred.shape[0], niou, dtype=torch.bool, device=device)\n",
    "    if nl:\n",
    "        detected = []\n",
    "        tcls_tensor = labels[:, 0]\n",
    "\n",
    "        # boxes\n",
    "        tbox = xywh2xyxy(labels[:, 1:5])\n",
    "        scale_coords(imgs[si].shape[1:], tbox, shapes[si][0], shapes[si][1])\n",
    "\n",
    "        # Per target class\n",
    "        for cls in torch.unique(tcls_tensor):\n",
    "            ti = (cls == tcls_tensor).nonzero(as_tuple=False).view(-1)\n",
    "            pi = (cls == pred[:, 5]).nonzero(as_tuple=False).view(-1) \n",
    "\n",
    "            # Search for detections\n",
    "            if pi.shape[0]:\n",
    "                ious, i = box_iou(predn[pi, :4], tbox[ti]).max(1)  # best ious, indices\n",
    "\n",
    "                # Append detections\n",
    "                detected_set = set()\n",
    "                for j in (ious > iouv[0]).nonzero(as_tuple=False):\n",
    "                    d = ti[i[j]]  \n",
    "                    if d.item() not in detected_set:\n",
    "                        detected_set.add(d.item())\n",
    "                        detected.append(d)\n",
    "                        correct[pi[j]] = ious[j] > iouv  \n",
    "                        if len(detected) == nl: \n",
    "                            break\n",
    "\n",
    "        # Append statistics (correct, conf, pcls, tcls)\n",
    "        stats.append((correct.cpu(), pred[:, 4].cpu(), pred[:, 5].cpu(), tcls))\n",
    "\n",
    "stats = [np.concatenate(x, 0) for x in zip(*stats)] \n",
    "if len(stats) and stats[0].any():\n",
    "    p, r, ap, f1, ap_class = ap_per_class(*stats)\n",
    "    ap50, ap = ap[:, 0], ap.mean(1) \n",
    "    mp, mr, map50, map = p.mean(), r.mean(), ap50.mean(), ap.mean()\n",
    "    nt = np.bincount(stats[3].astype(np.int64), minlength=NUMBER_OF_CLASSES)\n",
    "\n",
    "    general_test_results.append({\n",
    "    \"precision\": p,\n",
    "    \"recall\": r,\n",
    "    \"ap\": ap,\n",
    "    \"f1\": f1,\n",
    "    \"ap_class\": ap_class,\n",
    "    \"ap\": ap,\n",
    "    \"ap50\": ap50,\n",
    "    \"mp\": mp,\n",
    "    \"mr\": mr,\n",
    "    \"map50\": map50,\n",
    "    \"map\": map\n",
    "})\n",
    "\n",
    "else:\n",
    "    nt = torch.zeros(1)\n",
    "\n",
    "# Print results\n",
    "pf = '%20s' + '%12i' * 2 + '%12.3g' * 4 \n",
    "print(pf % ('all', seen, nt.sum(), mp, mr, map50, map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2f4ddf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml",
   "language": "python",
   "name": "aml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

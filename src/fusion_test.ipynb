{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60af1d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./yolo')\n",
    "from detection_fusion import EnsembleModel\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from rcnn.dataset import ChestCocoDetection\n",
    "from torchvision import transforms\n",
    "import matplotlib.patches as patches\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffecaa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bounding_boxes(target, ax):\n",
    "    boxes = target['boxes']\n",
    "    for box in boxes:\n",
    "        mp_box = patches.Rectangle((box[0], box[1]), box[2] - box[0], box[3] - box[1], edgecolor=\"r\", facecolor='none')\n",
    "        ax.add_patch(mp_box)\n",
    "\n",
    "def show_samples_for(test, train):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10,8))\n",
    "    #fig.suptitle(f'Study: {samples[\"id\"].iloc[i]}')\n",
    "    for im_i, (img, data) in enumerate([test, train]):\n",
    "        ax = axs[im_i] if isinstance(axs, np.ndarray) else axs\n",
    "        ax.set_title(f'Instance / Image Nr. {im_i + 1} / {2}')\n",
    "        #dcm = pydicom.dcmread(image_path)\n",
    "        plt.figure()\n",
    "        ax.imshow(img.permute(1,2,0))#,cmap=plt.cm.bone)\n",
    "        add_bounding_boxes(data, ax)\n",
    "    fig.savefig(\"combo.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89adcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ChestCocoDetection(root=\"F:\\\\aml-project\\data\\\\siim-covid19-detection\", ann_file=\"F:\\\\aml-project\\\\data\\\\siim-covid19-detection\\\\test.json\", training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642b7530",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(len(test))\n",
    "test_img = test.__getitem__(idx)\n",
    "print(type(test_img[0]), test_img[0].shape)\n",
    "print(test_img[1])\n",
    "img = test_img[0]\n",
    "img_pil = transforms.ToPILImage()(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505bce10",
   "metadata": {},
   "outputs": [],
   "source": [
    "results, boxes, scores = detection_fusion(img_pil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcf7422",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_samples_for(test_img, (test_img[0], {\"boxes\":boxes}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0089df",
   "metadata": {},
   "source": [
    "### Evaluation of Ensemble method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73bcb799",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolo.utils.general import xywh2xyxy, scale_coords, box_iou\n",
    "from yolo.utils.metrics import ap_per_class\n",
    "import torch\n",
    "from rcnn.model import ChestRCNN\n",
    "from yolo.yolo import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb6cbc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    imgs, targets = zip(*batch)\n",
    "    imgs = torch.stack(imgs)\n",
    "    return tuple((imgs, targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ecd1b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# need bs = 1 because model fusion cannot handle batches \n",
    "test = ChestCocoDetection(root=\"F:\\\\aml-project\\data\\\\siim-covid19-detection\", ann_file=\"F:\\\\aml-project\\\\data\\\\siim-covid19-detection\\\\test.json\", training=False)\n",
    "test_loader = DataLoader(test, batch_size=1, shuffle=False, pin_memory=False, num_workers=0, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea068a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\aml\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "YOLO_FINAL_MODEL_PATH = \"./yolo/models_final_giou_40/yolov5_epoch_25.pt\"\n",
    "FASTER_RCNN_FINAL_MODEL_PATH = \"./rcnn/models/fasterrcnn_epoch_23.pt\"\n",
    "RESNET_BACKBONE_PATH = \"./resnet/models/resnext101_32x8d_epoch_35.pt\"\n",
    "\n",
    "yolov5_weights = torch.load(YOLO_FINAL_MODEL_PATH)\n",
    "fasterrcnn_r101_weights = torch.load(FASTER_RCNN_FINAL_MODEL_PATH)\n",
    "\n",
    "yolo = Model(cfg=\"./yolo/yolo5l.yaml\",ch=3,nc=1)\n",
    "yolo.load_state_dict(yolov5_weights, strict=False) \n",
    "\n",
    "fasterRcnn = ChestRCNN(RESNET_BACKBONE_PATH)\n",
    "fasterRcnn.load_state_dict(fasterrcnn_r101_weights)\n",
    "ensemble = EnsembleModel(fasterRcnn=fasterRcnn, yolo=yolo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3237cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|███▊                                                                           | 41/859 [06:17<2:02:16,  8.97s/it]"
     ]
    }
   ],
   "source": [
    "jdict, stats, ap, ap_class = [], [], [], []\n",
    "iouv = torch.linspace(0.5, 0.55, 1) # iou vector for mAP@0.5:0.95\n",
    "niou = iouv.numel()\n",
    "general_test_results = {}\n",
    "for i, (images, targets) in enumerate(tqdm.tqdm(test_loader)):\n",
    "    nb, _, height, width = images.shape\n",
    "    assert nb == 1\n",
    "    img_pil = transforms.ToPILImage()(images[0])\n",
    "    _, boxes, scores, pred_labels, _, _, _, _ = ensemble.detection_fusion(img_pil, extended_output=True)\n",
    "    boxes = torch.tensor(boxes)\n",
    "    gt_boxes = targets[0]['boxes']\n",
    "    labels = targets[0]['labels']\n",
    "    nl = len(labels)\n",
    "    tcls = labels[:].tolist() if nl else []  # target class\n",
    "\n",
    "    if len(boxes) == 0:\n",
    "        if nl:\n",
    "            stats.append((torch.zeros(0, niou, dtype=torch.bool), torch.Tensor(), torch.Tensor(), tcls))\n",
    "        continue\n",
    "\n",
    "    correct = torch.zeros(len(boxes), niou, dtype=torch.bool)\n",
    "    if nl:\n",
    "        detected = []\n",
    "        tcls_tensor = labels[:]\n",
    "\n",
    "        # Per target class\n",
    "        for cls in torch.tensor([1]):\n",
    "            ti = (cls == tcls_tensor).nonzero(as_tuple=False).view(-1)\n",
    "            pi = (cls == torch.tensor(pred_labels)).nonzero(as_tuple=False).view(-1) \n",
    "\n",
    "            # Search for detections\n",
    "            if pi.shape[0]:\n",
    "                ious, i = box_iou(boxes[pi], gt_boxes[ti]).max(1)  # best ious, indices\n",
    "\n",
    "                # Append detections\n",
    "                detected_set = set()\n",
    "                for j in (ious > iouv[0]).nonzero(as_tuple=False):\n",
    "                    d = ti[i[j]]  \n",
    "                    if d.item() not in detected_set:\n",
    "                        detected_set.add(d.item())\n",
    "                        detected.append(d)\n",
    "                        correct[pi[j]] = ious[j] > iouv  \n",
    "                        if len(detected) == nl: \n",
    "                            break\n",
    "\n",
    "        # Append statistics (correct, conf, pcls, tcls)\n",
    "        #print((correct.cpu(), torch.tensor(scores),torch.tensor(pred_labels), tcls))\n",
    "        stats.append((correct.cpu(), torch.tensor(scores),torch.tensor(pred_labels), tcls))\n",
    "\n",
    "stats = [np.concatenate(x, 0) for x in zip(*stats)] \n",
    "if len(stats) and stats[0].any():\n",
    "    p, r, ap, f1, ap_class = ap_per_class(*stats)\n",
    "    ap50, ap = ap[:, 0], ap.mean(1) \n",
    "    mp, mr, map50, map = p.mean(), r.mean(), ap50.mean(), ap.mean()\n",
    "    nt = np.bincount(stats[3].astype(np.int64), minlength=1)\n",
    "\n",
    "    general_test_results.append({\n",
    "    \"precision\": p,\n",
    "    \"recall\": r,\n",
    "    \"ap\": ap,\n",
    "    \"f1\": f1,\n",
    "    \"ap_class\": ap_class,\n",
    "    \"ap\": ap,\n",
    "    \"ap50\": ap50,\n",
    "    \"mp\": mp,\n",
    "    \"mr\": mr,\n",
    "    \"map50\": map50,\n",
    "    \"map\": map\n",
    "})\n",
    "\n",
    "else:\n",
    "    nt = torch.zeros(1)\n",
    "\n",
    "# Print results\n",
    "pf = '%20s' + '%12i' * 2 + '%12.3g' * 4 \n",
    "print(pf % ('all', seen, nt.sum(), mp, mr, map50, map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c353024",
   "metadata": {},
   "outputs": [],
   "source": [
    "map50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d3e6e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml",
   "language": "python",
   "name": "aml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import sys\r\n",
    "sys.path.append('./yolo')\r\n",
    "from detection_fusion import EnsembleModel\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from rcnn.dataset import ChestCocoDetection\r\n",
    "import matplotlib.patches as patches\r\n",
    "from torch.utils.data import DataLoader\r\n",
    "import tqdm\r\n",
    "import torch"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from yolo.utils.general import xywh2xyxy, scale_coords, box_iou\r\n",
    "from yolo.utils.metrics import ap_per_class\r\n",
    "import torch\r\n",
    "from rcnn.model import ChestRCNN\r\n",
    "from yolo.yolo import Model\r\n",
    "from yolo.utils.datasets import LoadImages\r\n",
    "from yolo.utils.general import check_img_size"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "YOLO_FINAL_MODEL_PATH = \"./yolo/models_final/yolov5_epoch_26.pt\"\r\n",
    "FASTER_RCNN_FINAL_MODEL_PATH = \"./rcnn/models/fasterrcnn_epoch_23.pt\"\r\n",
    "RESNET_BACKBONE_PATH = \"./resnet/models/resnext101_32x8d_epoch_35.pt\"\r\n",
    "\r\n",
    "yolov5_weights = torch.load(YOLO_FINAL_MODEL_PATH)\r\n",
    "fasterrcnn_r101_weights = torch.load(FASTER_RCNN_FINAL_MODEL_PATH)\r\n",
    "\r\n",
    "yolo = Model(cfg=\"./yolo/yolo5l.yaml\",ch=3,nc=1)\r\n",
    "yolo.load_state_dict(yolov5_weights, strict=False) \r\n",
    "\r\n",
    "fasterRcnn = ChestRCNN(RESNET_BACKBONE_PATH)\r\n",
    "fasterRcnn.load_state_dict(fasterrcnn_r101_weights)\r\n",
    "ensemble = EnsembleModel(fasterRcnn=fasterRcnn, yolo=yolo)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation of Ensemble method"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "def collate_fn(batch):\r\n",
    "    imgs, targets, path = zip(*batch)\r\n",
    "    imgs = torch.stack(imgs)\r\n",
    "    return tuple((imgs, targets, path))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "# need bs = 1 because model fusion cannot handle batches \r\n",
    "test = ChestCocoDetection(root=\"F:\\\\aml-project\\data\\\\siim-covid19-detection\", ann_file=\"F:\\\\aml-project\\\\data\\\\siim-covid19-detection\\\\test.json\", training=False, image_size=512, detection_fusion=True)\r\n",
    "test_loader = DataLoader(test, batch_size=1, shuffle=False, pin_memory=False, num_workers=0, collate_fn=collate_fn)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "VISUALIZATION = True\r\n",
    "jdict, stats, ap, ap_class = [], [], [], []\r\n",
    "stats_yolo, stats_fcnn = [], []\r\n",
    "iouv = torch.linspace(0.5, 0.55, 1) # iou vector for mAP@0.5:0.95\r\n",
    "niou = iouv.numel()\r\n",
    "general_test_results = []\r\n",
    "visualization_sample_data = {}\r\n",
    "for i, (images, targets, path) in enumerate(tqdm.tqdm(test_loader)):\r\n",
    "    nb, _, height, width = images.shape\r\n",
    "    assert nb == 1\r\n",
    "\r\n",
    "    # Load image with Yolo transformations since we use the FCNN data loader per default\r\n",
    "    path = '.\\..\\\\data\\\\siim-covid19-detection\\\\' + path[0]\r\n",
    "    stride = int(yolo.stride.max())  # model stride\r\n",
    "    img_size = check_img_size(512, s=stride)  # check img_size\r\n",
    "    yolo_img = LoadImages(path,img_size=img_size,stride=stride).__iter__().__next__()[1]\r\n",
    "    yolo_img = torch.from_numpy(yolo_img).float()\r\n",
    "    yolo_img /= 255.0\r\n",
    "    if yolo_img.ndimension() == 3:\r\n",
    "         yolo_img = yolo_img.unsqueeze(0)\r\n",
    "\r\n",
    "    _, boxes, scores, pred_labels, yolo_box, yolo_scores, yolo_labels, fcnn_box, fcnn_scores, fcnn_labels = ensemble.detection_fusion(images,yolo_img, extended_output=True)\r\n",
    "    boxes = torch.tensor(boxes)\r\n",
    "    yolo_box = torch.tensor(yolo_box)\r\n",
    "    fcnn_box = torch.tensor(fcnn_box)\r\n",
    "    gt_boxes = targets[0]['boxes']\r\n",
    "    labels = targets[0]['labels']\r\n",
    "    nl = len(labels)\r\n",
    "    tcls = labels[:].tolist() if nl else []  # target class\r\n",
    "\r\n",
    "    # save one sample prediction for visualization\r\n",
    "    if len(boxes) and i == 5:\r\n",
    "        visualization_sample_data[\"img\"] = images\r\n",
    "        visualization_sample_data[\"gt_box\"] = gt_boxes\r\n",
    "        visualization_sample_data[\"yolo_box\"] = yolo_box\r\n",
    "        visualization_sample_data[\"yolo_scores\"] = yolo_scores\r\n",
    "        visualization_sample_data[\"fcnn_box\"] = fcnn_box\r\n",
    "        visualization_sample_data[\"fcnn_scores\"] = fcnn_scores\r\n",
    "        visualization_sample_data[\"ensemble_box\"] = boxes\r\n",
    "        visualization_sample_data[\"ensemble_scores\"] = scores\r\n",
    "        if VISUALIZATION:\r\n",
    "            break\r\n",
    "\r\n",
    "\r\n",
    "    if len(boxes) == 0:\r\n",
    "        if nl:\r\n",
    "            stats.append((torch.zeros(0, niou, dtype=torch.bool), torch.Tensor(), torch.Tensor(), tcls))\r\n",
    "            stats_yolo.append((torch.zeros(0, niou, dtype=torch.bool), torch.Tensor(), torch.Tensor(), tcls))\r\n",
    "            stats_fcnn.append((torch.zeros(0, niou, dtype=torch.bool), torch.Tensor(), torch.Tensor(), tcls))\r\n",
    "\r\n",
    "        continue\r\n",
    "\r\n",
    "    correct = torch.zeros(len(boxes), niou, dtype=torch.bool)\r\n",
    "    correct_yolo = torch.zeros(len(yolo_box), niou, dtype=torch.bool)\r\n",
    "    correct_fcnn = torch.zeros(len(fcnn_box), niou, dtype=torch.bool)\r\n",
    "    \r\n",
    "    if nl:\r\n",
    "        detected = []\r\n",
    "        detected_yolo = []\r\n",
    "        detected_fcnn = []\r\n",
    "        tcls_tensor = labels[:]\r\n",
    "\r\n",
    "        # Per target class\r\n",
    "        for cls in torch.tensor([1]):\r\n",
    "            ti = (cls == tcls_tensor).nonzero(as_tuple=False).view(-1)\r\n",
    "            pi = (cls == torch.tensor(pred_labels)).nonzero(as_tuple=False).view(-1)\r\n",
    "            pi_yolo = (cls == torch.tensor(yolo_labels)).nonzero(as_tuple=False).view(-1)\r\n",
    "            pi_fcnn = (cls == torch.tensor(fcnn_labels)).nonzero(as_tuple=False).view(-1)\r\n",
    "\r\n",
    "            # Search for detections\r\n",
    "            if pi.shape[0]:        \r\n",
    "                ious, i = box_iou(boxes[pi], gt_boxes[ti]).max(1)  # best ious, indices\r\n",
    "                # Append detections - Ensemble\r\n",
    "                detected_set = set()\r\n",
    "                for j in (ious > iouv[0]).nonzero(as_tuple=False):\r\n",
    "                    d = ti[i[j]]  \r\n",
    "                    if d.item() not in detected_set:\r\n",
    "                        detected_set.add(d.item())\r\n",
    "                        detected.append(d)\r\n",
    "                        correct[pi[j]] = ious[j] > iouv  \r\n",
    "                        if len(detected) == nl: \r\n",
    "                            break\r\n",
    "                            \r\n",
    "            if pi_yolo.shape[0]:\r\n",
    "                ious_yolo, i_yolo = box_iou(yolo_box[pi_yolo], gt_boxes[ti]).max(1)\r\n",
    "                # Yolo\r\n",
    "                detected_set_yolo = set()\r\n",
    "                for j in (ious_yolo > iouv[0]).nonzero(as_tuple=False):\r\n",
    "                    d = ti[i_yolo[j]]  \r\n",
    "                    if d.item() not in detected_set_yolo:\r\n",
    "                        detected_set_yolo.add(d.item())\r\n",
    "                        detected_yolo.append(d)\r\n",
    "                        correct_yolo[pi_yolo[j]] = ious_yolo[j] > iouv  \r\n",
    "                        if len(detected_yolo) == nl: \r\n",
    "                            break\r\n",
    "                            \r\n",
    "            if pi_fcnn.shape[0]:\r\n",
    "                ious_fcnn, i_fcnn = box_iou(fcnn_box[pi_fcnn], gt_boxes[ti]).max(1)\r\n",
    "                # Faster R-CNN\r\n",
    "                detected_set_fcnn = set()\r\n",
    "                for j in (ious_fcnn > iouv[0]).nonzero(as_tuple=False):\r\n",
    "                    d = ti[i_fcnn[j]]  \r\n",
    "                    if d.item() not in detected_set_fcnn:\r\n",
    "                        detected_set_fcnn.add(d.item())\r\n",
    "                        detected_fcnn.append(d)\r\n",
    "                        correct_fcnn[pi_fcnn[j]] = ious_fcnn[j] > iouv  \r\n",
    "                        if len(detected_fcnn) == nl: \r\n",
    "                            break\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "        # Append statistics (correct, conf, pcls, tcls)\r\n",
    "        stats.append((correct.cpu(), torch.tensor(scores),torch.tensor(pred_labels), tcls))\r\n",
    "        stats_yolo.append((correct_yolo.cpu(), torch.tensor(yolo_scores),torch.tensor(yolo_labels), tcls))\r\n",
    "        stats_fcnn.append((correct_fcnn.cpu(), torch.tensor(fcnn_scores),torch.tensor(fcnn_labels), tcls))\r\n",
    "\r\n",
    "for statistic in [stats, stats_yolo, stats_fcnn]:\r\n",
    "    statistic = [np.concatenate(x, 0) for x in zip(*statistic)] \r\n",
    "    if len(statistic) and statistic[0].any():\r\n",
    "        p, r, ap, f1, ap_class = ap_per_class(*statistic)\r\n",
    "        ap50, ap = ap[:, 0], ap.mean(1) \r\n",
    "        mp, mr, map50, m = p.mean(), r.mean(), ap50.mean(), ap.mean()\r\n",
    "        nt = np.bincount(statistic[3].astype(np.int64), minlength=1)\r\n",
    "\r\n",
    "        general_test_results.append({\r\n",
    "        \"precision\": p,\r\n",
    "        \"recall\": r,\r\n",
    "        \"ap\": ap,\r\n",
    "        \"f1\": f1,\r\n",
    "        \"ap_class\": ap_class,\r\n",
    "        \"ap\": ap,\r\n",
    "        \"ap50\": ap50,\r\n",
    "        \"mp\": mp,\r\n",
    "        \"mr\": mr,\r\n",
    "        \"map50\": map50,\r\n",
    "        \"map\": m\r\n",
    "    })\r\n",
    "\r\n",
    "    else:\r\n",
    "        nt = torch.zeros(1)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  1%|          | 5/859 [00:23<1:05:44,  4.62s/it]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# Ensemble\r\n",
    "general_test_results[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'precision': array([    0.59357]),\n",
       " 'recall': array([     0.5302]),\n",
       " 'ap': array([    0.45424]),\n",
       " 'f1': array([     0.5601]),\n",
       " 'ap_class': array([1]),\n",
       " 'ap50': array([    0.45424]),\n",
       " 'mp': 0.5935737555211111,\n",
       " 'mr': 0.5301970756516211,\n",
       " 'map50': 0.454241758431956,\n",
       " 'map': 0.454241758431956}"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# YOLO\r\n",
    "general_test_results[1]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'precision': array([    0.50593]),\n",
       " 'recall': array([    0.45048]),\n",
       " 'ap': array([    0.34455]),\n",
       " 'f1': array([    0.47659]),\n",
       " 'ap_class': array([1]),\n",
       " 'ap50': array([    0.34455]),\n",
       " 'mp': 0.505925645110543,\n",
       " 'mr': 0.4504762082687064,\n",
       " 'map50': 0.34455343562825014,\n",
       " 'map': 0.34455343562825014}"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "# Faster R-CNN\r\n",
    "general_test_results[2]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'precision': array([    0.60309]),\n",
       " 'recall': array([    0.50328]),\n",
       " 'ap': array([    0.40929]),\n",
       " 'f1': array([    0.54868]),\n",
       " 'ap_class': array([1]),\n",
       " 'ap50': array([    0.40929]),\n",
       " 'mp': 0.6030946457346565,\n",
       " 'mr': 0.5032776105936924,\n",
       " 'map50': 0.40928837471026075,\n",
       " 'map': 0.40928837471026075}"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "visualization_sample_data"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'img': tensor([[[[-1.63841, -1.65554, -1.65554,  ..., -1.91241, -1.77541, -1.67266],\n",
       "           [-1.56991, -1.70691, -1.75828,  ..., -1.80966, -1.87816, -1.80966],\n",
       "           [-1.68979, -1.70691, -1.77541,  ..., -1.84391, -1.91241, -1.91241],\n",
       "           ...,\n",
       "           [-1.80966, -1.92953, -1.98091,  ..., -1.15892, -1.09042, -1.00479],\n",
       "           [-1.72403, -1.98091, -1.99803,  ..., -1.07329, -1.07329, -0.98767],\n",
       "           [-1.89528, -1.80966, -1.89528,  ..., -1.21029, -1.12467, -1.05617]],\n",
       " \n",
       "          [[-1.54552, -1.56303, -1.56303,  ..., -1.82563, -1.68557, -1.58053],\n",
       "           [-1.47549, -1.61555, -1.66807,  ..., -1.72059, -1.79062, -1.72059],\n",
       "           [-1.59804, -1.61555, -1.68557,  ..., -1.75560, -1.82563, -1.82563],\n",
       "           ...,\n",
       "           [-1.72059, -1.84314, -1.89566,  ..., -1.05532, -0.98529, -0.89776],\n",
       "           [-1.63305, -1.89566, -1.91317,  ..., -0.96779, -0.96779, -0.88025],\n",
       "           [-1.80812, -1.72059, -1.80812,  ..., -1.10784, -1.02031, -0.95028]],\n",
       " \n",
       "          [[-1.31643, -1.33386, -1.33386,  ..., -1.59529, -1.45586, -1.35129],\n",
       "           [-1.24671, -1.38614, -1.43843,  ..., -1.49072, -1.56044, -1.49072],\n",
       "           [-1.36871, -1.38614, -1.45586,  ..., -1.52558, -1.59529, -1.59529],\n",
       "           ...,\n",
       "           [-1.49072, -1.61272, -1.66501,  ..., -0.82841, -0.75869, -0.67155],\n",
       "           [-1.40357, -1.66501, -1.68244,  ..., -0.74126, -0.74126, -0.65412],\n",
       "           [-1.57786, -1.49072, -1.57786,  ..., -0.88070, -0.79355, -0.72383]]]]),\n",
       " 'gt_box': tensor([[120.03747,  69.43726, 268.41046, 280.94153],\n",
       "         [303.45663,  79.01481, 423.66168, 325.63678]]),\n",
       " 'yolo_box': tensor([[315.13600,  75.26400, 426.08640, 335.66720],\n",
       "         [118.63040,  66.35520, 247.29600, 281.24160]], dtype=torch.float64),\n",
       " 'yolo_scores': [0.5013, 0.3277],\n",
       " 'fcnn_box': tensor([[302.13120,  73.62560, 435.96800, 370.07360],\n",
       "         [112.17920,  64.46080, 251.08480, 295.93600],\n",
       "         [171.36640, 206.38720, 227.73760, 325.12000]], dtype=torch.float64),\n",
       " 'fcnn_scores': [0.9352, 0.8019, 0.2452],\n",
       " 'ensemble_box': tensor([[306.66959,  74.19740, 432.51959, 358.06671],\n",
       "         [114.05070,  65.01040, 249.98570, 291.67310],\n",
       "         [171.36639, 206.38721, 227.73759, 325.12000]]),\n",
       " 'ensemble_scores': [0.7182499766349792,\n",
       "  0.5648000240325928,\n",
       "  0.1225999966263771]}"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "def add_bounding_boxes(target, ax):\r\n",
    "    boxes = target['boxes']\r\n",
    "    print(boxes.shape)\r\n",
    "    for box in boxes:\r\n",
    "        mp_box = patches.Rectangle((box[0], box[1]), box[2] - box[0], box[3] - box[1], edgecolor=\"r\", facecolor='none')\r\n",
    "        ax.add_patch(mp_box)\r\n",
    "\r\n",
    "def show_samples_for(gt, yolo, fcnn, ensemble):\r\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(12,8))\r\n",
    "    for im_i, (img, box, name) in enumerate([gt, yolo, fcnn, ensemble]):\r\n",
    "        ax = axs[im_i] if isinstance(axs, np.ndarray) else axs\r\n",
    "        ax.set_title(name)\r\n",
    "        plt.figure()\r\n",
    "        ax.imshow(img.squeeze_(0).permute(1,2,0))#,cmap=plt.cm.bone)\r\n",
    "        add_bounding_boxes(box, ax)\r\n",
    "    \r\n",
    "    fig.savefig(\"combo.png\",bbox_inches='tight', dpi=200)\r\n",
    "\r\n",
    "\r\n",
    "show_samples_for(\r\n",
    "    (visualization_sample_data['img'], {\"boxes\":visualization_sample_data['gt_box']}, \"Ground Truth\"), \r\n",
    "    (visualization_sample_data['img'], {\"boxes\":visualization_sample_data['yolo_box']}, \"YOLO\"),\r\n",
    "    (visualization_sample_data['img'], {\"boxes\":visualization_sample_data['fcnn_box']}, \"Faster R-CNN\"),\r\n",
    "    (visualization_sample_data['img'], {\"boxes\":visualization_sample_data['ensemble_box']}, \"Ensemble\")\r\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 4])\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('aml': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "interpreter": {
   "hash": "e2c56da327a3544e8df7760b824cf0444fbff4a4b0d34d9cc09299f87e272f6b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import sys\r\n",
    "sys.path.append('./yolo')\r\n",
    "from detection_fusion import EnsembleModel\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from rcnn.dataset import ChestCocoDetection\r\n",
    "import matplotlib.patches as patches\r\n",
    "from torch.utils.data import DataLoader\r\n",
    "import tqdm\r\n",
    "import torch"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from yolo.utils.general import xywh2xyxy, scale_coords, box_iou\r\n",
    "from yolo.utils.metrics import ap_per_class\r\n",
    "import torch\r\n",
    "from rcnn.model import ChestRCNN\r\n",
    "from yolo.yolo import Model\r\n",
    "from yolo.utils.datasets import LoadImages\r\n",
    "from yolo.utils.general import check_img_size"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "YOLO_FINAL_MODEL_PATH = \"./yolo/models_final/yolov5_epoch_26.pt\"\r\n",
    "FASTER_RCNN_FINAL_MODEL_PATH = \"./rcnn/models/fasterrcnn_epoch_23.pt\"\r\n",
    "RESNET_BACKBONE_PATH = \"./resnet/models/resnext101_32x8d_epoch_35.pt\"\r\n",
    "\r\n",
    "yolov5_weights = torch.load(YOLO_FINAL_MODEL_PATH)\r\n",
    "fasterrcnn_r101_weights = torch.load(FASTER_RCNN_FINAL_MODEL_PATH)\r\n",
    "\r\n",
    "yolo = Model(cfg=\"./yolo/yolo5l.yaml\",ch=3,nc=1)\r\n",
    "yolo.load_state_dict(yolov5_weights, strict=False) \r\n",
    "\r\n",
    "fasterRcnn = ChestRCNN(RESNET_BACKBONE_PATH)\r\n",
    "fasterRcnn.load_state_dict(fasterrcnn_r101_weights)\r\n",
    "ensemble = EnsembleModel(fasterRcnn=fasterRcnn, yolo=yolo)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "D:\\Anaconda\\envs\\aml\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation of Ensemble method"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def collate_fn(batch):\r\n",
    "    imgs, targets, path = zip(*batch)\r\n",
    "    imgs = torch.stack(imgs)\r\n",
    "    return tuple((imgs, targets, path))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# need bs = 1 because model fusion cannot handle batches \r\n",
    "test = ChestCocoDetection(root=\"F:\\\\aml-project\\data\\\\siim-covid19-detection\", ann_file=\"F:\\\\aml-project\\\\data\\\\siim-covid19-detection\\\\test.json\", training=False, image_size=512, detection_fusion=True)\r\n",
    "test_loader = DataLoader(test, batch_size=1, shuffle=False, pin_memory=False, num_workers=0, collate_fn=collate_fn)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "VISUALIZATION = True\r\n",
    "jdict, stats, ap, ap_class = [], [], [], []\r\n",
    "stats_yolo, stats_fcnn = [], []\r\n",
    "iouv = torch.linspace(0.5, 0.55, 1) # iou vector for mAP@0.5:0.95\r\n",
    "niou = iouv.numel()\r\n",
    "general_test_results = []\r\n",
    "selected_samples = [5, 10, 18, 32, 26]\r\n",
    "visualization_sample_data = {\"img\": [], \"gt_box\": [], \"yolo_box\": [], \"yolo_scores\": [], \"fcnn_box\": [], \"fcnn_scores\": [], \"ensemble_box\": [], \"ensemble_scores\": [] }\r\n",
    "for i, (images, targets, path) in enumerate(tqdm.tqdm(test_loader)):\r\n",
    "    nb, _, height, width = images.shape\r\n",
    "    assert nb == 1\r\n",
    "\r\n",
    "    # Load image with Yolo transformations since we use the FCNN data loader per default\r\n",
    "    path = '.\\..\\\\data\\\\siim-covid19-detection\\\\' + path[0]\r\n",
    "    stride = int(yolo.stride.max())  # model stride\r\n",
    "    img_size = check_img_size(512, s=stride)  # check img_size\r\n",
    "    yolo_img = LoadImages(path,img_size=img_size,stride=stride).__iter__().__next__()[1]\r\n",
    "    yolo_img = torch.from_numpy(yolo_img).float()\r\n",
    "    yolo_img /= 255.0\r\n",
    "    if yolo_img.ndimension() == 3:\r\n",
    "         yolo_img = yolo_img.unsqueeze(0)\r\n",
    "\r\n",
    "    _, boxes, scores, pred_labels, yolo_box, yolo_scores, yolo_labels, fcnn_box, fcnn_scores, fcnn_labels = ensemble.detection_fusion(images,yolo_img, extended_output=True)\r\n",
    "    boxes = torch.tensor(boxes)\r\n",
    "    yolo_box = torch.tensor(yolo_box)\r\n",
    "    fcnn_box = torch.tensor(fcnn_box)\r\n",
    "    gt_boxes = targets[0]['boxes']\r\n",
    "    labels = targets[0]['labels']\r\n",
    "    nl = len(labels)\r\n",
    "    tcls = labels[:].tolist() if nl else []  # target class\r\n",
    "\r\n",
    "    # save one sample prediction for visualization\r\n",
    "    if len(boxes) and i in selected_samples :\r\n",
    "        visualization_sample_data[\"img\"].append(images)\r\n",
    "        visualization_sample_data[\"gt_box\"].append(gt_boxes)\r\n",
    "        visualization_sample_data[\"yolo_box\"].append(yolo_box)\r\n",
    "        visualization_sample_data[\"yolo_scores\"].append(yolo_scores)\r\n",
    "        visualization_sample_data[\"fcnn_box\"].append(fcnn_box)\r\n",
    "        visualization_sample_data[\"fcnn_scores\"].append(fcnn_scores)\r\n",
    "        visualization_sample_data[\"ensemble_box\"].append(boxes)\r\n",
    "        visualization_sample_data[\"ensemble_scores\"].append(scores)\r\n",
    "        \r\n",
    "        if VISUALIZATION and len(selected_samples) == len(visualization_sample_data[\"img\"]):\r\n",
    "            break\r\n",
    "\r\n",
    "\r\n",
    "    if len(boxes) == 0:\r\n",
    "        if nl:\r\n",
    "            stats.append((torch.zeros(0, niou, dtype=torch.bool), torch.Tensor(), torch.Tensor(), tcls))\r\n",
    "            stats_yolo.append((torch.zeros(0, niou, dtype=torch.bool), torch.Tensor(), torch.Tensor(), tcls))\r\n",
    "            stats_fcnn.append((torch.zeros(0, niou, dtype=torch.bool), torch.Tensor(), torch.Tensor(), tcls))\r\n",
    "\r\n",
    "        continue\r\n",
    "\r\n",
    "    correct = torch.zeros(len(boxes), niou, dtype=torch.bool)\r\n",
    "    correct_yolo = torch.zeros(len(yolo_box), niou, dtype=torch.bool)\r\n",
    "    correct_fcnn = torch.zeros(len(fcnn_box), niou, dtype=torch.bool)\r\n",
    "    \r\n",
    "    if nl:\r\n",
    "        detected = []\r\n",
    "        detected_yolo = []\r\n",
    "        detected_fcnn = []\r\n",
    "        tcls_tensor = labels[:]\r\n",
    "\r\n",
    "        # Per target class\r\n",
    "        for cls in torch.tensor([1]):\r\n",
    "            ti = (cls == tcls_tensor).nonzero(as_tuple=False).view(-1)\r\n",
    "            pi = (cls == torch.tensor(pred_labels)).nonzero(as_tuple=False).view(-1)\r\n",
    "            pi_yolo = (cls == torch.tensor(yolo_labels)).nonzero(as_tuple=False).view(-1)\r\n",
    "            pi_fcnn = (cls == torch.tensor(fcnn_labels)).nonzero(as_tuple=False).view(-1)\r\n",
    "\r\n",
    "            # Search for detections\r\n",
    "            if pi.shape[0]:        \r\n",
    "                ious, i = box_iou(boxes[pi], gt_boxes[ti]).max(1)  # best ious, indices\r\n",
    "                # Append detections - Ensemble\r\n",
    "                detected_set = set()\r\n",
    "                for j in (ious > iouv[0]).nonzero(as_tuple=False):\r\n",
    "                    d = ti[i[j]]  \r\n",
    "                    if d.item() not in detected_set:\r\n",
    "                        detected_set.add(d.item())\r\n",
    "                        detected.append(d)\r\n",
    "                        correct[pi[j]] = ious[j] > iouv  \r\n",
    "                        if len(detected) == nl: \r\n",
    "                            break\r\n",
    "                            \r\n",
    "            if pi_yolo.shape[0]:\r\n",
    "                ious_yolo, i_yolo = box_iou(yolo_box[pi_yolo], gt_boxes[ti]).max(1)\r\n",
    "                # Yolo\r\n",
    "                detected_set_yolo = set()\r\n",
    "                for j in (ious_yolo > iouv[0]).nonzero(as_tuple=False):\r\n",
    "                    d = ti[i_yolo[j]]  \r\n",
    "                    if d.item() not in detected_set_yolo:\r\n",
    "                        detected_set_yolo.add(d.item())\r\n",
    "                        detected_yolo.append(d)\r\n",
    "                        correct_yolo[pi_yolo[j]] = ious_yolo[j] > iouv  \r\n",
    "                        if len(detected_yolo) == nl: \r\n",
    "                            break\r\n",
    "                            \r\n",
    "            if pi_fcnn.shape[0]:\r\n",
    "                ious_fcnn, i_fcnn = box_iou(fcnn_box[pi_fcnn], gt_boxes[ti]).max(1)\r\n",
    "                # Faster R-CNN\r\n",
    "                detected_set_fcnn = set()\r\n",
    "                for j in (ious_fcnn > iouv[0]).nonzero(as_tuple=False):\r\n",
    "                    d = ti[i_fcnn[j]]  \r\n",
    "                    if d.item() not in detected_set_fcnn:\r\n",
    "                        detected_set_fcnn.add(d.item())\r\n",
    "                        detected_fcnn.append(d)\r\n",
    "                        correct_fcnn[pi_fcnn[j]] = ious_fcnn[j] > iouv  \r\n",
    "                        if len(detected_fcnn) == nl: \r\n",
    "                            break\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "        # Append statistics (correct, conf, pcls, tcls)\r\n",
    "        stats.append((correct.cpu(), torch.tensor(scores),torch.tensor(pred_labels), tcls))\r\n",
    "        stats_yolo.append((correct_yolo.cpu(), torch.tensor(yolo_scores),torch.tensor(yolo_labels), tcls))\r\n",
    "        stats_fcnn.append((correct_fcnn.cpu(), torch.tensor(fcnn_scores),torch.tensor(fcnn_labels), tcls))\r\n",
    "\r\n",
    "for statistic in [stats, stats_yolo, stats_fcnn]:\r\n",
    "    statistic = [np.concatenate(x, 0) for x in zip(*statistic)] \r\n",
    "    if len(statistic) and statistic[0].any():\r\n",
    "        p, r, ap, f1, ap_class = ap_per_class(*statistic)\r\n",
    "        ap50, ap = ap[:, 0], ap.mean(1) \r\n",
    "        mp, mr, map50, m = p.mean(), r.mean(), ap50.mean(), ap.mean()\r\n",
    "        nt = np.bincount(statistic[3].astype(np.int64), minlength=1)\r\n",
    "\r\n",
    "        general_test_results.append({\r\n",
    "        \"precision\": p,\r\n",
    "        \"recall\": r,\r\n",
    "        \"ap\": ap,\r\n",
    "        \"f1\": f1,\r\n",
    "        \"ap_class\": ap_class,\r\n",
    "        \"ap\": ap,\r\n",
    "        \"ap50\": ap50,\r\n",
    "        \"mp\": mp,\r\n",
    "        \"mr\": mr,\r\n",
    "        \"map50\": map50,\r\n",
    "        \"map\": m\r\n",
    "    })\r\n",
    "\r\n",
    "    else:\r\n",
    "        nt = torch.zeros(1)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  4%|â–Ž         | 32/859 [02:10<56:22,  4.09s/it]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Ensemble\r\n",
    "general_test_results[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'precision': array([    0.59357]),\n",
       " 'recall': array([     0.5302]),\n",
       " 'ap': array([    0.45424]),\n",
       " 'f1': array([     0.5601]),\n",
       " 'ap_class': array([1]),\n",
       " 'ap50': array([    0.45424]),\n",
       " 'mp': 0.5935737555211111,\n",
       " 'mr': 0.5301970756516211,\n",
       " 'map50': 0.454241758431956,\n",
       " 'map': 0.454241758431956}"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# YOLO\r\n",
    "general_test_results[1]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'precision': array([    0.50593]),\n",
       " 'recall': array([    0.45048]),\n",
       " 'ap': array([    0.34455]),\n",
       " 'f1': array([    0.47659]),\n",
       " 'ap_class': array([1]),\n",
       " 'ap50': array([    0.34455]),\n",
       " 'mp': 0.505925645110543,\n",
       " 'mr': 0.4504762082687064,\n",
       " 'map50': 0.34455343562825014,\n",
       " 'map': 0.34455343562825014}"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "# Faster R-CNN\r\n",
    "general_test_results[2]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'precision': array([    0.60309]),\n",
       " 'recall': array([    0.50328]),\n",
       " 'ap': array([    0.40929]),\n",
       " 'f1': array([    0.54868]),\n",
       " 'ap_class': array([1]),\n",
       " 'ap50': array([    0.40929]),\n",
       " 'mp': 0.6030946457346565,\n",
       " 'mr': 0.5032776105936924,\n",
       " 'map50': 0.40928837471026075,\n",
       " 'map': 0.40928837471026075}"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "class UnNormalize(object):\r\n",
    "    def __init__(self, mean, std):\r\n",
    "        self.mean = mean\r\n",
    "        self.std = std\r\n",
    "\r\n",
    "    def __call__(self, tensor):\r\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\r\n",
    "            t.mul_(s).add_(m)\r\n",
    "        return tensor\r\n",
    "\r\n",
    "\r\n",
    "unorm = UnNormalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\r\n",
    "imgs = []\r\n",
    "for i in visualization_sample_data['img']:\r\n",
    "    imgs.append(unorm(i))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "import matplotlib.gridspec as gridspec\r\n",
    "nrow = 5\r\n",
    "ncol = 4\r\n",
    "\r\n",
    "def add_bounding_boxes(boxes, ax):\r\n",
    "    for box in boxes:\r\n",
    "        mp_box = patches.Rectangle((box[0], box[1]), box[2] - box[0], box[3] - box[1], edgecolor=\"r\", facecolor='none')\r\n",
    "        ax.add_patch(mp_box)\r\n",
    "\r\n",
    "def show_samples_for(gt, yolo, fcnn, ensemble):\r\n",
    "    fig = plt.figure(figsize=(6, 10), constrained_layout=True) \r\n",
    "    fig.subplots_adjust(wspace=0, hspace=0)\r\n",
    "    gs = gridspec.GridSpec(5, 4, width_ratios=[1, 1, 1, 1],\r\n",
    "         wspace=0.1, hspace=0.1) \r\n",
    "\r\n",
    "    for im_i, (img, box, name) in enumerate([gt, yolo, fcnn, ensemble]):\r\n",
    "        ax = plt.subplot(gs[0, im_i])\r\n",
    "        ax.set_title(name)\r\n",
    "        for im_j, (i, b) in enumerate(zip(img, box)):\r\n",
    "            ax = plt.subplot(gs[im_j,im_i])\r\n",
    "            ax.set_xticklabels([])\r\n",
    "            ax.set_yticklabels([])\r\n",
    "            ax.imshow(i.squeeze_(0).permute(1,2,0))\r\n",
    "            add_bounding_boxes(b, ax)           \r\n",
    "\r\n",
    "    fig.savefig(\"combo.png\",bbox_inches='tight', dpi=200)\r\n",
    "\r\n",
    "\r\n",
    "show_samples_for(\r\n",
    "    (imgs, visualization_sample_data['gt_box'], \"Ground Truth\"), \r\n",
    "    (imgs, visualization_sample_data['yolo_box'], \"YOLO\"),\r\n",
    "    (imgs, visualization_sample_data['fcnn_box'], \"Faster R-CNN\"),\r\n",
    "    (imgs, visualization_sample_data['ensemble_box'], \"Ensemble\")\r\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Julian\\AppData\\Local\\Temp/ipykernel_18872/2475878709.py:12: UserWarning: This figure was using constrained_layout, but that is incompatible with subplots_adjust and/or tight_layout; disabling constrained_layout.\n",
      "  fig.subplots_adjust(wspace=0, hspace=0)\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('aml': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "interpreter": {
   "hash": "e2c56da327a3544e8df7760b824cf0444fbff4a4b0d34d9cc09299f87e272f6b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}